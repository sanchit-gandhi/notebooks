{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf87589-b7fe-45dd-a6f6-9b9223581562",
   "metadata": {},
   "source": [
    "# Speculative Decoding for 2x Faster Whisper Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7260efa-ef8b-43e5-a0d0-213c58ab90d7",
   "metadata": {},
   "source": [
    "Open AI's [Whisper](https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013) is a general purpose speech transcription model that achieves state-of-the-art results across a range of different benchmarks and audio conditions. The latest [large-v3](https://huggingface.co/openai/whisper-large-v3) model tops the [OpenASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard), ranking as the best open-source speech transcription model for English. The model also demonstrates strong multilingual performance, acheiving less than 30% word error rate (WER) on 42 of the 58 languages tested in the Common Voice 15 dataset.\n",
    "\n",
    "While the transcription accuracy is exceptional, the inference time is very slow. A 1 hour audio clip takes upwards of 6 minutes to transcribe on a 16GB T4 GPU, even after leveraging inference optimisations like [flash attention](https://huggingface.co/openai/whisper-large-v3#flash-attention), half-precision and chunking. In this Google Colab, we demonstrate how Speculative Decoding can be employed to reduce the inference time of Whisper by a **factor of 2**, while mathematically ensuring exactly the **same outputs** are achieved from the model. As a result, this method provides a perfect drop-in replacement for existing Whisper pipelines, since it provides free 2x speed-up while maintaining the same accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d9e08c-8cb5-428c-b823-6b28e7fbe72f",
   "metadata": {},
   "source": [
    "## Speculative Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b3757-72dc-48a8-9d9d-fc135386cae5",
   "metadata": {},
   "source": [
    "Speculative Decoding was proposed in the paper [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192) by Yaniv Leviathan et. al. from Google.\n",
    "It works on the premise that a faster, **assistant model** can be used to boostrap the generation of a larger, **main model**.\n",
    "\n",
    "First, the assistant model auto-regressively generates a sequence of $N$ *candidate tokens*, $\\hat{\\boldsymbol{y}}_{1:N}$. In the diagram below, the assistant model generates a sequence of 5 candidate tokens: `The quick brown sock jumps`.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 70%; margin: auto;\"\n",
    "        controls playsinline\n",
    "        src=\"https://huggingface.co/datasets/sanchit-gandhi/notebook-figures/resolve/main/speculative-decoding/granular/split_1.mp4\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "While these candidate tokens are generated quickly, they may differ from those predicted by the main model. Therefore, in the second step, the candidate tokens are passed to the main model to be \"verified\". The main model takes the candidate tokens as input and performs a **single forward pass**. The outputs of the main model are the \"correct\" token for each step in the token sequence $\\boldsymbol{y}_{1:N}$.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 70%; margin: auto;\"\n",
    "        controls playsinline\n",
    "        src=\"https://huggingface.co/datasets/sanchit-gandhi/notebook-figures/resolve/main/speculative-decoding/granular/split_2.mp4\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "In the diagram above, we see that the first three tokens predicted by the main model agree with those from the assistant model: <span style=\"color:green\">The quick brown</span>. However, the fourth candidate token from the assistant model, <span style=\"color:red\">sock</span>, mismatches with the correct token from the main model, <span style=\"color:green\">fox</span>.\n",
    "\n",
    "We know that all candidate tokens up to the first mismatch are correct (<span style=\"color:green\">The quick brown</span>), since these agree with the predictions from the main model. However, after the first mismatch, the candidate tokens diverge from the actual tokens predicted by the main model. Therefore, we can replace the first incorrect candidate token (<span style=\"color:red\">sock</span>) with the correct token from the main model (<span style=\"color:green\">fox</span>), and discard all predicted tokens that come after this, since these have diverged. The corrected sequence, `The quick brown fox`, now forms the new input to the assistant model:\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 70%; margin: auto;\"\n",
    "        controls playsinline\n",
    "        src=\"https://huggingface.co/datasets/sanchit-gandhi/notebook-figures/resolve/main/speculative-decoding/granular/split_3.mp4\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "The inference process then repeats, the assistant model generating a new set of $N$ candidate tokens, which are verified in a single forward pass by the main model.\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 70%; margin: auto;\"\n",
    "        controls playsinline\n",
    "        src=\"https://huggingface.co/datasets/sanchit-gandhi/notebook-figures/resolve/main/speculative-decoding/granular/split_4.mp4\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "Since we auto-regressively generate using the fast, assistant model, and only perform verification forward passes with the slow, main model, the decoding process is sped-up substantially. Furthermore, the verification forward passes performed by the main model ensure that **exactly the same outputs** are achieved as using the main model standalone. This makes speculative decoding the perfect substitute to existing Whisper pipelines, since one can be certain that the same performance will be attained.\n",
    "\n",
    "To get the biggest speed-up in latency, we want the assistant model to be as fast as possible, while predicting the same token distribution as the main model. In practice, these two attributes form a trade-off: the faster the model is, the less accurate it is. The only constraint for selecting an assistant model is that it must share the same vocabulary as the main model. That is to say, if we want to use speculative decoding with Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) (multilingual), we need to select a multilingual variant of Whisper. Whereas if we want to use speculative decoding with Whisper [medium.en](https://huggingface.co/openai/whisper-medium.en) (English-only), we need an English-only variant of Whisper. At the current time, Whisper [large-v3](https://huggingface.co/openai/whisper-large-v3) is an exception, since it is the only Whisper checkpoint with an expanded vocabulary size, and thus is not compatible with previous Whisper checkpoints.\n",
    "\n",
    "Now that we know the background behind speculative decoding, we're ready to dive into the practical implementation. In the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library, speculative decoding is implemented as the \"assisted generation\" inference strategy. \n",
    "For more details about the implementation and speculative decoding in general, the reader is advised to read Joao Gante's excellent blog post on [Assisted Generation](https://huggingface.co/blog/assisted-generation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0023abe-cdb3-4360-a195-31ef8d75a372",
   "metadata": {},
   "source": [
    "## English Speech Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176ab50-29c3-43b8-ac39-95c15f72a328",
   "metadata": {},
   "source": [
    "We start by benchmarking Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) to get our baseline number for inference speed. We can load the main model and it's corresponding processor via the convenient [`AutoModelForSpeechSeq2Seq`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForSpeechSeq2Seq) and [`AutoProcessor`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoProcessor) classes. We'll load the model in `float16` precision and make sure that loading time takes as little time as possible by passing [`low_cpu_mem_usage=True`](https://huggingface.co/docs/transformers/main_classes/model#large-model-loading). In addition, we want to make sure that the model is loaded in [safetensors](https://huggingface.co/docs/diffusers/main/en/using-diffusers/using_safetensors) format by passing [`use_safetensors=True`](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.use_safetensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa759449-a3e8-40d4-b6b0-04a99c52ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sanchit/transformers/src/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "2023-12-05 16:49:34.230055: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 16:49:34.230087: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 16:49:34.230107: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 16:49:34.926842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v2\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54153a-39d2-4999-b92e-68965aba7d59",
   "metadata": {},
   "source": [
    "We can export the model to [BetterTransformers](https://huggingface.co/docs/optimum/bettertransformer/overview) format to benefit from Flash Attention speed-ups through PyTorch's [SDPA attention kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2948f6-2bb9-4124-9a50-8b0e9eaae4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "model = model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a0d25-5c44-4a8c-8812-a01a2970c63c",
   "metadata": {},
   "source": [
    "Let's load the English speech transcription dataset that we will use for benchmarking. We'll load a small dataset consisting of 73 samples from the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) validation-clean dataset. This amounts to ~9MB of data, so it's very lightweight and quick to download on device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e22b7922-2dd1-4d74-85d9-1ebfe03e53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278faded-58ba-4db2-b927-1699414ca23d",
   "metadata": {},
   "source": [
    "For the benchmark, we only want to measure the generation time, so let's write a short helper function that measures this step. The following function will return both the decoded tokens as well as the time it took to run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3816e5-9c62-44c7-b807-41dcd62c6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_with_time(model, inputs, *kwargs):\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, **kwargs)\n",
    "    generation_time = time.time() - start_time\n",
    "    return outputs, generation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2375302-74a7-4b39-8cbc-ac124991e4ea",
   "metadata": {},
   "source": [
    "We can now iterate over the audio samples and sum up the overall generation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32f92ac6-cef6-46ac-b180-df672de02ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:35<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.839271068572998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "all_time = 0\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    audio = sample[\"audio\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device=device, dtype=torch.float16)\n",
    "    \n",
    "    output, gen_time = generate_with_time(model, inputs)\n",
    "    all_time += gen_time\n",
    "    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])\n",
    "    references.append(processor.tokenizer._normalize(sample[\"text\"]))\n",
    "\n",
    "print(all_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64184941-edd4-49a2-a73a-ebff8ed76f3d",
   "metadata": {},
   "source": [
    "Alright! We see that transcribing the 73 samples took TODO seconds. Let's check the WER of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "770a9f5f-8865-4951-b8d6-3e86818ebfd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03507271171941831\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer = load(\"wer\")\n",
    "\n",
    "print(wer.compute(predictions=predictions, references=references))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc9847-011d-41d6-9382-4920907ddd3d",
   "metadata": {},
   "source": [
    "Our final baseline numbers are TODO seconds for a WER of 3.5%.\n",
    "\n",
    "Now let's load the assistant model for speculative decoding. In this example, we'll use a distilled variant of Whisper, [distil-large-v2](https://huggingface.co/distil-whisper/distil-large-v2). The distilled model copies the entire encoder from Whisper, but only 2 of the 32 decoder layers. As such, it runs 6x faster than Whisper, while performing to within 1% WER on our-of-distribution test sets. This makes it the perfect candidate choice of assistant model, since it has both high transcription accuracy and fast generation<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1).\n",
    "\n",
    "Since Distil-Whisper uses exactly same encoder as the Whisper model, we can share the encoder across the main and assistant models. We then only have to load the 2-layer decoder from Distil-Whisper as a \"decoder-only\" model. We can do this through the convinient [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) auto class. In practice, this results in only an 8% increase to VRAM over using the main model alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3838db5-c6d5-4270-bca9-8492dad15673",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "The transformation of the model WhisperForCausalLM to BetterTransformer failed while it should not. Please fill a bug report or open a PR to support this model at https://github.com/huggingface/optimum/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m assistant_model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     assistant_model_id, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, use_safetensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m assistant_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m assistant_model \u001b[38;5;241m=\u001b[39m \u001b[43massistant_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bettertransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/transformers/src/transformers/modeling_utils.py:4129\u001b[0m, in \u001b[0;36mPreTrainedModel.to_bettertransformer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   4124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install optimum>=1.7.0 to use Better Transformer. The version \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4125\u001b[0m     )\n\u001b[1;32m   4127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptimum\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbettertransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BetterTransformer\n\u001b[0;32m-> 4129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBetterTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/optimum/optimum/bettertransformer/transformation.py:266\u001b[0m, in \u001b[0;36mBetterTransformer.transform\u001b[0;34m(model, keep_original_model, max_memory, offload_dir, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m BetterTransformerManager\u001b[38;5;241m.\u001b[39mrequires_nested_tensor(model_fast\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type):\n\u001b[0;32m--> 266\u001b[0m     \u001b[43mset_last_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Add a class arguments, we might need to identify whether the model\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# has been correctly converted to its `BetterTransformer` version.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28msetattr\u001b[39m(model_fast, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_bettertransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/optimum/optimum/bettertransformer/transformation.py:166\u001b[0m, in \u001b[0;36mset_last_layer\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(dict_named_module[key][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    164\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transformation of the model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to BetterTransformer failed while it should not. Please fill\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m a bug report or open a PR to support this model at https://github.com/huggingface/optimum/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m )\n",
      "\u001b[0;31mException\u001b[0m: The transformation of the model WhisperForCausalLM to BetterTransformer failed while it should not. Please fill a bug report or open a PR to support this model at https://github.com/huggingface/optimum/"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "assistant_model_id = \"distil-whisper/distil-large-v2\"\n",
    "\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "assistant_model.to(device)\n",
    "assistant_model = assistant_model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9efdb-7256-4c0d-a8e7-1576b864ff9b",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. We intend to release an improved variant of Distil-Whisper with a stronger alignment in the token distribution that will improve speculative decoding performance further. Follow the [Distil-Whisper repository](https://github.com/huggingface/distil-whisper) for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4514e-f646-432a-87ea-affb3290a5ff",
   "metadata": {},
   "source": [
    "We can define a modified function for our speculative decoding benchmark. The only difference from the previous function is that we pass the assistant model to our call to `.generate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9e5f82c-0965-460e-b4e4-e07e8bac8a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assisted_generate_with_time(model, inputs, **kwargs):\n",
    "    start_time = time.time()\n",
    "    outputs = model.generate(**inputs, assistant_model=assistant_model, **kwargs)\n",
    "    generation_time = time.time() - start_time\n",
    "    return outputs, generation_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd94af-f381-4a38-9a21-d16934085b94",
   "metadata": {},
   "source": [
    "Let's run the benchmark with speculative decoding, using Distil-Whisper as the assistant to Whisper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd4e44e-d247-4f9e-9add-cb62e3ef90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:21<00:00,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.212965250015259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_time = 0\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    audio = sample[\"audio\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device=device, dtype=torch.float16)\n",
    "    \n",
    "    output, gen_time = assisted_generate_with_time(model, inputs)\n",
    "    all_time += gen_time\n",
    "    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])\n",
    "    references.append(processor.tokenizer._normalize(sample[\"text\"]))\n",
    "\n",
    "print(all_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64314fcc-435c-4e19-84c2-aaeca451580a",
   "metadata": {},
   "source": [
    "With speculative decoding, the inference time just TODO seconds, a factor of 2x reduction! Let's verify we have the same WER as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f91466e-b30b-4eb5-86c6-b811668940fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03507271171941831\n"
     ]
    }
   ],
   "source": [
    "print(wer.compute(predictions=predictions, references=references))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb0ea3b-fabb-416e-9358-c6521d94a123",
   "metadata": {},
   "source": [
    "Perfect! 3.5% WER again. This confirms we have identical outputs to using the main model standalone.\n",
    "\n",
    "Speculative decoding can also be incorporated with ðŸ¤— Transformers [pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) class for an easy API for inference. Below, we instantiate the pipeline using the model and processor, and then use it to transcribe the first sample from the toy dataset. This can be extended to transcribe audio samples of arbritrary length, including with the use of batching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbeacf8c-c0d8-41c6-bb4c-323a34a2a562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=15,\n",
    "    batch_size=4,\n",
    "    generate_kwargs={\"assistant_model\": assistant_model},\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "sample = dataset[0][\"audio\"]\n",
    "result = pipe(sample)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10a377e-140b-4f6b-9b36-e5da9ac99c93",
   "metadata": {},
   "source": [
    "An end-to-end codesnippet for running speculative decoding with Whisper and Distil-Whisper can be found on the [Distil-Whisper model card](https://huggingface.co/distil-whisper/distil-large-v2#speculative-decoding). It combines the stages of inference covered in this notebook into a single code example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e35a03a-aac5-43c8-87da-8e5a90cee39c",
   "metadata": {},
   "source": [
    "##Â Multilingual Speech Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8526121-a913-4a89-bfdf-0b14b42c2982",
   "metadata": {},
   "source": [
    "Distil-Whisper is the perfect assistant model for English speech transcription, since it performs to within 1% WER of the original Whisper model, while being 6x faster over short and long-form audio samples. However, the official Distil-Whisper checkpoints are English only, meaning they cannot be used for multilingual speech transcription. To use speculative decoding for multilingual speech transcription, one could either use on of the [official multilingual Whisper checkpoints](https://huggingface.co/openai/whisper-large-v2#model-details), or a fine-tuned variant of Whisper. As of the time of writing, there are over 5,000 [fine-tuned Whisper checkpoints](https://huggingface.co/models?other=whisper) on the Hugging Face Hub in over 100 languages. These provide an excellent starting point for selecting assistant Whisper checkpoints that perform very well on a single language. In this example, we'll use the smallest official multilingual checkpoint, Whisper [tiny](https://huggingface.co/openai/whisper-tiny)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159d77db-41b9-4978-8224-0d8ff9d2579c",
   "metadata": {},
   "source": [
    "Let's load the weights for our new assistant model, Whisper tiny. Since the encoder in Whisper tiny differs from that in large-v2, we'll load both the encoder and decoder using the `AutoModelForSpeechSeq2Seq` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e362a99-65aa-40d5-af68-eed0252f31b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "assistant_model_id = \"openai/whisper-tiny\"\n",
    "\n",
    "assistant_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    assistant_model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "\n",
    "assistant_model.to(device)\n",
    "assistant_model = assistant_model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98a6ec-ef2f-42d1-a34a-e632774da92d",
   "metadata": {},
   "source": [
    "For our benchmarking dataset, we'll load 73 samples from the Dutch (nl) split of the [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) datset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47aa3832-3918-43ab-9518-24f19aed1040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 999/999 [00:00<00:00, 4.58MB/s]\n",
      "Downloading data files:   0%|                                                                     | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|                                                                     | 0.00/30.1M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                   | 4.19M/30.1M [00:00<00:04, 5.79MB/s]\u001b[A\n",
      "Downloading data:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 12.6M/30.1M [00:01<00:01, 11.2MB/s]\u001b[A\n",
      "Downloading data:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 21.0M/30.1M [00:01<00:00, 13.4MB/s]\u001b[A\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.1M/30.1M [00:02<00:00, 13.1MB/s]\u001b[A\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.30s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 628.27it/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:00<00:00, 348.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sanchit-gandhi/voxpopuli_dummy\", \"nl\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638903df-64fd-4eb2-b403-6d53cf4dfa7a",
   "metadata": {},
   "source": [
    "Great! We can now re-run our benchmark for our baseline Whisper large-v2 model as before. The only change we make is that we pass the language and task arguments to our generate function, in order to ensure we perform speech transcription (not speech translation). Note that speculative decoding is fully compatible with the speech translation task. Simply set the task argument as required below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d96ebfc-ecfb-4a3f-8e68-3e56c6c140f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:53<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 47.14989519119263\n",
      "WER: 0.12783906554185595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_time = 0\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    audio = sample[\"audio\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device=device, dtype=torch.float16)\n",
    "    \n",
    "    output, gen_time = generate_with_time(model, inputs, language=\"nl\", task=\"transcribe\")\n",
    "    all_time += gen_time\n",
    "    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])\n",
    "    references.append(processor.tokenizer._normalize(sample[\"normalized_text\"]))\n",
    "\n",
    "wer_result = wer.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Time:\", all_time)\n",
    "print(\"WER:\", wer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300eaa65-95a1-4a6c-8f54-8e3e4d925386",
   "metadata": {},
   "source": [
    "Right! We have our baseline time of TODO seconds and a WER of 12.8%. Let's re-run the generation process using speculative decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bc5e280-d78e-4aa8-9eee-6199ff3d4ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [00:36<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 29.686880588531494\n",
      "WER: 0.12654120700843607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_time = 0\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    audio = sample[\"audio\"]\n",
    "    inputs = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device=device, dtype=torch.float16)\n",
    "    \n",
    "    output, gen_time = assisted_generate_with_time(model, inputs, language=\"nl\", task=\"transcribe\")\n",
    "    all_time += gen_time\n",
    "    predictions.append(processor.batch_decode(output, skip_special_tokens=True, normalize=True)[0])\n",
    "    references.append(processor.tokenizer._normalize(sample[\"normalized_text\"]))\n",
    "\n",
    "wer_result = wer.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"Time:\", all_time)\n",
    "print(\"WER:\", wer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090f372-d5d8-4cca-94b2-72500b7bf661",
   "metadata": {},
   "source": [
    "Again, we achieve 12.8% WER, but this time in just TODO seconds of inference time, representing a speed-up of TODOx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab7147-d426-47a9-8bf3-29ecbc6b2c19",
   "metadata": {},
   "source": [
    "## Strategies for Efficient Speculative Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8e081-4626-4265-b93a-3c3e71a77c26",
   "metadata": {},
   "source": [
    "#### Assistant Model\n",
    "\n",
    "Our objective is to select an assistant model that is both fast **and** maintains the same token distribution as the main model. If you have a particular language in which you want to transcribe, an effective strategy is to train two Whisper models of different sizes, and use one as the assistant to the other:\n",
    "\n",
    "* Fine-tune Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) to act as your main model\n",
    "* Distil Whisper [large-v2](https://huggingface.co/openai/whisper-large-v2) on the same dataset to act as your assistant model\n",
    "\n",
    "Additional training improves the WER performance of both the main and assistant model on your chosen language, while maximising the alignment in the token distributions. A complete guide to Whisper fine-tuning can be found [here](https://huggingface.co/blog/fine-tune-whisper), and distillation [here](https://github.com/huggingface/distil-whisper/tree/main/training).\n",
    "\n",
    "####Â Batch Size\n",
    "\n",
    "It is worth noting that the largest speed gains with speculative decoding come with a batch size of 1. For batched speculative decoding, all candidate tokens **across the batch** must match the validation tokens in order for the tokens to be accepted. If a token in the batch at a given position does not agree, all candidate tokens that precede the position are discarded. Consequently, speculative decoding favours lower batch sizes. In practice, we find that speculative decoding provides a speed-up until a batch size of 4. Above batch size 4, speculative decoding returns slower inference than the main model alone. For full results, refer to Section D.3 of the [Distil-Whisper paper](https://arxiv.org/pdf/2311.00430.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b098e5-941c-4775-b9b7-f27d0dfacda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
