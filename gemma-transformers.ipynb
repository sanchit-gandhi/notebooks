{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0634464a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/gemma-transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
   "metadata": {
    "id": "bfef752c-1a66-4240-bacf-c6432ab77354"
   },
   "source": [
    "#Â Gemma in ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57289a71-58f3-4801-8595-27585c300392",
   "metadata": {
    "id": "57289a71-58f3-4801-8595-27585c300392"
   },
   "source": [
    "Gemma is a family language models released by Google DeepMind in the paper [Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf). Gemma leverages the same research and technology used to create the Gemini models. It was trained using a dataset consisting of 6-trillion tokens, formed from web documents, code and mathematics. The result is a series of state-of-the-art models at the 2B and 7B scale, all of which are open-sourced and permissively licensed.\n",
    "\n",
    "Gemma has support in the ðŸ¤— Transformers library from day-0, in both PyTorch and JAX. In this Google Colab, we'll showcase how to use the Gemma 7B model in PyTorch, leveraging the familiar Transformers 3-line API. By quantising the model to 4-bit precision, the notebook can be run on a Google Colab T4 free-tier GPU. We'll wrap the loaded model into a Gradio demo, showcasing how the model can be shared with anyone in the community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LIxx0Gt0AY72",
   "metadata": {
    "id": "LIxx0Gt0AY72"
   },
   "source": [
    "##Â Set-Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0NOicgrIBMo_",
   "metadata": {
    "id": "0NOicgrIBMo_"
   },
   "source": [
    "First, we need to register our Hugging Face Hub token with our Google Colab runtime. Since the Gemma model is gated, our token will be checked when the model is downloaded to ensure we have accepted the terms-of-use. To register your token, click the key symbol ðŸ”‘ in the left-hand pane of the screen. Name the secret `HF_TOKEN`, and copy a token from your Hugging Face [Hub account](https://huggingface.co/settings/tokens). Your token should now be registered, allowing you to access the Gemma weights to this Colab session.\n",
    "\n",
    "For reasonable inference speed with Gemma, we'll want to run the model on a GPU. The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all you need to do is hit `\"Connect T4\"` in the top right-hand corner of the screen.\n",
    "\n",
    "Once we've done that, we can go ahead and install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vN61p-AEBSP3",
   "metadata": {
    "id": "vN61p-AEBSP3"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet transformers accelerate bitsandbytes gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b178891-40ce-4055-9d64-fbf860fc4278",
   "metadata": {
    "id": "8b178891-40ce-4055-9d64-fbf860fc4278"
   },
   "source": [
    "##Â Baseline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9810b5d-0507-4d9e-bbca-d22a9e8b4942",
   "metadata": {
    "id": "f9810b5d-0507-4d9e-bbca-d22a9e8b4942"
   },
   "source": [
    "In this example, we'll use Transformers' model + tokenizer API. Let's go ahead an import the two Python classes we'll need to run the Gemma model. The first of these classes, [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM), is our model class. This is the Python class that holds the model weights and graph definition. The second class, [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer), is the tokenizer class, which converts the input string to a token id (discrete number) representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d9474-6b18-41e7-9d53-f9741bc5cbfa",
   "metadata": {
    "id": "433d9474-6b18-41e7-9d53-f9741bc5cbfa"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893926-b201-445f-b548-10b5ed643ab1",
   "metadata": {
    "id": "62893926-b201-445f-b548-10b5ed643ab1"
   },
   "source": [
    "We can now load the pre-trained model weights from the Hugging Face Hub. There are four pre-trained Gemma checkpoints from which we can choose from, summarised in the table below:\n",
    "\n",
    "| Model ID    | Size / B params | Type        |\n",
    "|-------------|-----------------|-------------|\n",
    "| [gemma-2b](https://huggingface.co/google/gemma-2b)    | 2.5             | Base        |\n",
    "| [gemma-2b-it](https://huggingface.co/google/gemma-2b-it) | 2.5             | Instruction |\n",
    "| [gemma-7b](https://huggingface.co/google/gemma-7b)    | 8.5             | Base        |\n",
    "| [gemma-7b-it](https://huggingface.co/google/gemma-7b-it) | 8.5             | Instruction |\n",
    "\n",
    "\n",
    "For this example, we'll use [gemma-7b-it](https://huggingface.co/google/gemma-7b-it), the 8.5B parameter model that has been instructioned tuned for the task of an assistant (or chatbot). An 8.5B parameter LLM in full precision (float32) requires 16GB of memory just to load the weights. The GPU typically assigned to a Google Colab free tier only has a capacity of 16GB. This means we are at risk of an *out-of-memory (OOM)* error, where the memory of the model exceeds that of the GPU. To circumvent this, we'll load the weights in [4-bit precision](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which reduces the memory of the weights roughly by a factor of 8. This is extremely simple in Transformers: we just pass the flag [`load_in_4bit`](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.load_in_4bit) when we load the pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15",
   "metadata": {
    "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15"
   },
   "outputs": [],
   "source": [
    "checkpoint_id = \"google/gemma-7b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_id, low_cpu_mem_usage=True, load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecec8ce-0997-4b66-a98d-0b72da4c6bac",
   "metadata": {
    "id": "cecec8ce-0997-4b66-a98d-0b72da4c6bac"
   },
   "source": [
    "The advantage of using the `AutoModelForCausalLM` and `AutoTokenizer` API over model-specific classes is that we can easily swap the checkpoint id for any other checkpoint\n",
    "on the Hugging Face Hub and re-use our code without any changes. The auto-classes will take care of loading the correct model and tokenizer classes for us! That means if a new LLM is released, we can quickly update our code to leverage the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc086ea-aa85-444d-a3ee-11eb8f719f71",
   "metadata": {
    "id": "ffc086ea-aa85-444d-a3ee-11eb8f719f71"
   },
   "source": [
    "Great! We've loaded our model into memory, so now we're ready to define our pre-processing strategy. Our inputs consist of a series of user/assistant responses, from which we want to generate a new assistant response. Each instruction tuned model has it's own format for user/assistant responses, and it can be fiddly getting one's inputs into the correct format for the model. Luckily for us, the tokenizer provides a convinient method [`apply_chat_template`](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) to format the tokens correctly for us. We just need to define what messages were sent by the user, and what ones were provided by the assistant. The tokenizer will then handle the precise formatting, and getting the ids ready for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58984579-0043-4bae-81af-fc242e8f2306",
   "metadata": {
    "id": "58984579-0043-4bae-81af-fc242e8f2306"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me about current trends artificial intelligence?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Certainly! Artificial Intelligence (AI) is advancing rapidly, with key trends including the growing adoption of deep learning techniques, the increasing use of AI in healthcare, finance, retail, and other industries, and the development of new AI tools for creative tasks, like music composition and visual art creation. Additionally, there is a growing focus on ethical considerations surrounding AI, as concerns arise about potential bias, privacy implications, and job displacement. Furthermore, AI is integrating with other technologies, such as blockchain and virtual reality, to create novel solutions for various challenges.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What should I consider when using large language models?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398fc4f-c439-4050-a2e9-da807717318b",
   "metadata": {
    "id": "a398fc4f-c439-4050-a2e9-da807717318b"
   },
   "source": [
    "Perfect! Now, our model is currently sitting on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05466436-f050-45c8-9f7e-23e8b9acf74b",
   "metadata": {
    "id": "05466436-f050-45c8-9f7e-23e8b9acf74b"
   },
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6628ed-1d58-49bf-8f18-f5365ad34426",
   "metadata": {
    "id": "7c6628ed-1d58-49bf-8f18-f5365ad34426"
   },
   "source": [
    "Whereas our inputs are on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb986f71-a374-4606-88bd-f8aaff754f45",
   "metadata": {
    "id": "cb986f71-a374-4606-88bd-f8aaff754f45"
   },
   "outputs": [],
   "source": [
    "encodeds.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa895911-1fa0-4a44-9d6e-478eff4154ac",
   "metadata": {
    "id": "fa895911-1fa0-4a44-9d6e-478eff4154ac"
   },
   "source": [
    "Every PyTorch model expects the inputs to be on the same device as the model. Let's move the inputs to the correct device before generating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eb655f-d621-4a58-b400-dd78b1acf762",
   "metadata": {
    "id": "c1eb655f-d621-4a58-b400-dd78b1acf762"
   },
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e81e4-22d2-49ff-81a7-a1b3ecd4d47a",
   "metadata": {
    "id": "9e1e81e4-22d2-49ff-81a7-a1b3ecd4d47a"
   },
   "source": [
    "Now that we've pre-processed our inputs, we can generate our response using the model in much the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc53409-a5e5-4e3d-8129-209be763f04d",
   "metadata": {
    "id": "edc53409-a5e5-4e3d-8129-209be763f04d"
   },
   "outputs": [],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8fc47-4023-4120-86d7-4c31a12435bc",
   "metadata": {
    "id": "3af8fc47-4023-4120-86d7-4c31a12435bc"
   },
   "source": [
    "Looks like a very reasonable response! We can see the change between the user input and the assistant response marked by the `[INST]` and `[/INST]` tokens. This was handled automatically for us by the tokenizer. Note that we've generated with [*sampling*](https://huggingface.co/blog/how-to-generate#sampling), so there's an element of randomness in the generations, meaining each one will be different from the last. Feel free to re-run generation a few times to get a feel for the kind of variety that sampling brings.\n",
    "\n",
    "> **Note:** the responses generated by the model are purely for demonstration purposes. They should not be taken as financial recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85bc5e-faf6-4f35-b22f-212fdd3eda9e",
   "metadata": {
    "id": "0c85bc5e-faf6-4f35-b22f-212fdd3eda9e"
   },
   "source": [
    "## Streaming Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065190a3-4062-45f3-bf91-918e7411b8dc",
   "metadata": {
    "id": "065190a3-4062-45f3-bf91-918e7411b8dc"
   },
   "source": [
    "While the generated response looks good, we had to wait a significant amount of time for the model to finish generating. We know from our knowledge of Transformer models that they generate in an *auto-regressive* fashion, that is, one token at a time:\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "Rather than waiting for the entire text sequence to finish generating, we can print each token as it is generated. While this doesn't change the overall latency of the model (the total generation time is the same as before), the *percieved* latency is lower, since the first generated token is returned to the user as soon as it's ready.\n",
    "\n",
    "Transformers provides a [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer) class to easily print the predictions on-the-fly. We simply have to pass the streamer to our generate function, and the rest is taken care for us. Let's run an example using the text streamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7896d-160a-40f5-a5f0-72b382bea2d9",
   "metadata": {
    "id": "2be7896d-160a-40f5-a5f0-72b382bea2d9"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0aac2a-5458-4f33-a54c-f39378dd184c",
   "metadata": {
    "id": "9c0aac2a-5458-4f33-a54c-f39378dd184c"
   },
   "source": [
    "Interacting with the LLM this way already feels more personalised, as we watch it generate each token at a time. It also feels faster to the user, as we get the first output as soon as it is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ada74-36c1-49d1-9d64-7452af25488f",
   "metadata": {
    "id": "9c2ada74-36c1-49d1-9d64-7452af25488f"
   },
   "source": [
    "## Gradio Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86cf1d-49d6-4e92-b63b-0e492cc8fe4d",
   "metadata": {
    "id": "dd86cf1d-49d6-4e92-b63b-0e492cc8fe4d"
   },
   "source": [
    "While it's interesting for us learners to interact with the model through Python code, it's by no means the easiest way to share our model with others. To finish this section, we'll demonstrate how the Gemma model can be wrapped into a simple chat interface and shared with anyone in the community.\n",
    "\n",
    "To achieve this, we'll use the [Gradio library](https://www.gradio.app). Gradio is an open-source Python library that simplifies the creation of web interfaces for machine learning models. It allows developers and researchers to quickly build and share customisable UI components for model testing and feedback, enhancing the accessibility and usability of AI models.\n",
    "\n",
    "First, we'll define an end-to-end function that takes the user's message and the chat history, and generates a response. Largely speaking, this simply requires concatenating the three stages of prediction:\n",
    "1. Tokenizer (pre-process) the message and chat history\n",
    "2. Define the generation arguments\n",
    "3. Generate the predicted ids and stream the output\n",
    "\n",
    "We'll make a small modification to our streamer: instead of using the base [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer), we'll use the [`TextIteratorStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer), which returns the generated ids in a non-blocking way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bebe7-27da-4255-8429-bdcd0033b7b8",
   "metadata": {
    "id": "b62bebe7-27da-4255-8429-bdcd0033b7b8"
   },
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "MAX_INPUT_TOKEN_LENGTH = 4096\n",
    "\n",
    "def generate(message, chat_history):\n",
    "    # Step 1: pre-process the inputs\n",
    "    conversation = []\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "\n",
    "    # in-case our inputs exceed the maximum length, we might need to cut them\n",
    "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Step 2: define generation arguments\n",
    "    generate_kwargs = dict(\n",
    "        {\"input_ids\": input_ids},\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    # Step 3: generate and stream outputs\n",
    "    outputs = \"\"\n",
    "    for text in streamer:\n",
    "        outputs += text\n",
    "        yield outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5112d2-a068-4acc-838d-b07c1052b8c8",
   "metadata": {
    "id": "bc5112d2-a068-4acc-838d-b07c1052b8c8"
   },
   "source": [
    "With the generation function defined, we can now create a Gradio demo in just 3 lines of additional code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407a6d9-9eef-4a27-97c4-89dec1d10bad",
   "metadata": {
    "id": "d407a6d9-9eef-4a27-97c4-89dec1d10bad"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_interface = gr.ChatInterface(generate)\n",
    "chat_interface.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af8af6-f499-400d-a784-9277be0e6c5a",
   "metadata": {
    "id": "94af8af6-f499-400d-a784-9277be0e6c5a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
