{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e439c3d-9b28-493b-be08-74ecc207e8e7",
   "metadata": {},
   "source": [
    "## AudioLDM 2, but faster ⚡️\n",
    "\n",
    "AudioLDM 2 was proposed in [AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining](https://arxiv.org/abs/2308.05734) \n",
    "by Haohe Liu et al. AudioLDM 2 takes a text prompt as input and predicts the corresponding audio. It can generate realistic sound effects, human speech and music.\n",
    "\n",
    "While the generated audios are of high quality, running inference with the model is very slow: a single audio sample takes upwards of 30 seconds to generate, with a \n",
    "real-time factor of approximately 0.3 (1 second of audio takes 3 seconds to generate). This is due to a combination of factors, including a deep multi-stage modelling \n",
    "approach, large checkpoint sizes, and un-optimised code.\n",
    "\n",
    "In this Colab, we showcase how to use AudioLDM 2 in the Hugging Face 🧨 Diffusers library, exploring a range of code optimisations such as half-precision, flash attention,\n",
    "and compilation, and model optimisations such as scheduler choice and negative prompting, to reduce the inference time by over **10 times**, with minimal degradation in quality of the output audio.\n",
    "\n",
    "Read to the end to find out how to generate a 10 second audio sample in just 1 second, with a real-time factor of 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeb415c-ad03-4f7b-b718-d0411369f9d7",
   "metadata": {},
   "source": [
    "## Set-up environment\n",
    "\n",
    "Let’s make sure we’re connected to a GPU to run this notebook. To get a GPU, click `Runtime` -> `Change runtime type`, then change `Hardware accelerator` from `None` to `GPU`. We can verify that we’ve been assigned a GPU and view its specifications through the `nvidia-smi` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993efea-77e1-4faf-a30a-6f6f9c49168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8d447-93c4-453a-8292-30c2c6152c8f",
   "metadata": {},
   "source": [
    "We see here that we've got on Tesla T4 16GB GPU, although this may vary for you depending on GPU availablity and Colab GPU assignment.\n",
    "\n",
    "Next, we can install the required Python packages, namely 🧨 Diffusers for running the AudioLDM 2 diffusion process, and 🤗 Transformers for the CLAP, Flan-T5 and GPT2 models respectively. We'll install these packages from the `main` branch of their respective repositories, since AudioLDM 2 is not yet in the latest PyPi release:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5fc88-c72c-4045-9a7e-c4f16dc282cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade git+https://github.com/huggingface/diffusers.git git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4d73c-ea1e-494f-9453-360966d3324a",
   "metadata": {},
   "source": [
    "We'll also install the nightly version of PyTorch, to leverage the latest updates to [`torch.compile`](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9fd008-a276-4601-a1a7-8c2899b58768",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --pre torch torchaudio --index-url https://download.pytorch.org/whl/nightly/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8166ee-a8a7-4c01-833e-52ff556bd8d9",
   "metadata": {},
   "source": [
    "## Model overview\n",
    "\n",
    "Inspired by [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), AudioLDM 2\n",
    "is a text-to-audio _latent diffusion model (LDM)_ that learns continuous audio representations from text embeddings.\n",
    "\n",
    "The overall generation process is summarised as follows:\n",
    "\n",
    "1. Given a text input $\\boldsymbol{x}$, two text encoder models are used to compute the text embeddings: the text-branch of [CLAP](https://huggingface.co/docs/transformers/main/en/model_doc/clap), and the text-encoder of [Flan-T5](https://huggingface.co/docs/transformers/main/en/model_doc/flan-t5)\n",
    "\n",
    "$$\n",
    "\\boldsymbol{E}_{1} = \\text{CLAP}\\left(\\boldsymbol{x} \\right); \\quad \\boldsymbol{E}_{2} = \\text{T5}\\left(\\boldsymbol{x}\\right)\n",
    "$$\n",
    "\n",
    "2. These text embeddings are projected to a shared embedding space through individual linear projections:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{P}_{1} = \\boldsymbol{W}_{\\text{CLAP}} \\boldsymbol{E}_{1}; \\quad \\boldsymbol{P}_{2} = \\boldsymbol{W}_{\\text{T5}}\\boldsymbol{E}_{2}\n",
    "$$\n",
    "\n",
    "In the `diffusers` implementation, these projections are defined by the [AudioLDM2ProjectionModel](https://huggingface.co/docs/diffusers/api/pipelines/audioldm2/AudioLDM2ProjectionModel).\n",
    "\n",
    "3. A [GPT2](https://huggingface.co/docs/transformers/main/en/model_doc/gpt2) language model (LM) is used to auto-regressively generate a sequence of $N$ new embedding vectors, conditional on the projected CLAP and Flan-T5 embeddings:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{E}_{i} = \\text{GPT2}\\left(\\boldsymbol{P}_{1}, \\boldsymbol{P}_{2}, \\boldsymbol{E}_{1:i-1}\\right) \\qquad \\text{for } i=1,\\dots,N\n",
    "$$\n",
    "\n",
    "4. The generated embedding vectors $\\boldsymbol{E}_{1:N}$ and Flan-T5 text embeddings $\\boldsymbol{E}_{2}$ are used as cross-attention conditioning in the LDM, which *de-noises* \n",
    "a random latent via a reverse diffusion process. The LDM is run in the reverse diffusion process for a total of $T$ inference steps:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z}_{t} = \\text{LDM}\\left(\\boldsymbol{z}_{t-1} | \\boldsymbol{E}_{1:N}, \\boldsymbol{E}_{2}\\right) \\qquad \\text{for } t = 1, \\dots, T\n",
    "$$\n",
    "\n",
    "where the initial latent variable $\\boldsymbol{z}_{0}$ is drawn from a normal distribution $\\mathcal{N} \\left(\\boldsymbol{0}, \\boldsymbol{I} \\right)$. The [UNet](https://huggingface.co/docs/diffusers/api/pipelines/audioldm2/AudioLDM2UNet2DConditionModel) of the LDM is unique in \n",
    "the sense that it takes **two** sets of cross-attention embeddings, $\\boldsymbol{E}_{1:N}$ from the GPT2 langauge model, and $\\boldsymbol{E}_{2}$ from Flan-T5, as opposed to one cross-attention conditioning as in most other LDMs.\n",
    "\n",
    "5. The final de-noised latents $\\boldsymbol{z}_{T}$ are passed to the VAE decoder to recover the Mel spectrogram $\\boldsymbol{s}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{s} = \\text{VAE}_{\\text{dec}} \\left(\\boldsymbol{z}_{T}\\right)\n",
    "$$\n",
    "\n",
    "6. The Mel spectrogram is passed to the vocoder to obtain the output audio waveform $\\mathbf{y}$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{y} = \\text{Vocoder}\\left(\\boldsymbol{s}\\right)\n",
    "$$\n",
    "\n",
    "The diagram below demonstrates how a text input is passed through the text conditioning models, with the two prompt embeddings used as cross-conditioning in the LDM:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/audioldm2.png?raw=true\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "Hugging Face 🧨 Diffusers provides an end-to-end inference pipeline class [`AudioLDM2Pipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2) that wraps this multi-stage generation process into a single callable object, enabling you to generate audio samples from text in just a few lines of code. \n",
    "\n",
    "AudioLDM 2 comes in three variants. Two of these checkpoints are applicable to the general task of text-to-audio generation. The third checkpoint is trained exclusively on text-to-music generation. See the table below for details on the three official checkpoints, which can all be found on the [Hugging Face Hub](https://huggingface.co/models?search=cvssp/audioldm2):\n",
    "\n",
    "| Checkpoint                                                      | Task          | Model Size | Training Data / h |\n",
    "|-----------------------------------------------------------------|---------------|------------|-------------------|\n",
    "| [audioldm2](https://huggingface.co/cvssp/audioldm2)             | Text-to-audio | 1.1B       | 1150k             |\n",
    "| [audioldm2-music](https://huggingface.co/cvssp/audioldm2-music) | Text-to-music | 1.1B       | 665k              |\n",
    "| [audioldm2-large](https://huggingface.co/cvssp/audioldm2-large) | Text-to-audio | 1.5B       | 1150k             |\n",
    "\n",
    "Now that we've covered a high-level overview of how the AudioLDM 2 generation process works, let's put this theory into code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d275ae93-b268-467a-9b25-1f252d16f060",
   "metadata": {},
   "source": [
    "## Load the pipeline\n",
    "\n",
    "For the purposes of this tutorial, we'll initialise the pipeline with the pre-trained weights from the base checkpoint, [cvssp/audioldm2](https://huggingface.co/cvssp/audioldm2). We can load the entirety of the pipeline using the [`.from_pretrained`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained) method, which will instantiate the pipeline and load the pre-trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ebd91-f261-488c-b3f5-371f4eea2423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AudioLDM2Pipeline\n",
    "\n",
    "model_id = \"cvssp/audioldm2\"\n",
    "pipe = AudioLDM2Pipeline.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3539842-692d-41f1-a35c-4d2b4463ee2b",
   "metadata": {},
   "source": [
    "The pipeline can be moved to the GPU in much the same way as a standard PyTorch nn module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fe3108-ad19-448d-a7ce-5de81b125785",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312e57a-4588-4ce6-9693-ecf82f3d43ca",
   "metadata": {},
   "source": [
    "Great! We'll define a Generator and set a seed for reproducibility. This will allow us to tweak our prompts and observe the effect that they have on the generations by fixing the starting latents in the LDM model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953f5914-a1bc-4162-b024-e9867f33b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "generator = torch.Generator(\"cuda\").manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a3651-2696-4113-8058-fc6f707a757a",
   "metadata": {},
   "source": [
    "Now we're ready to perform our first generation! We'll use the same running example throughout this notebook, where we'll condition the audio generations on a fixed text prompt\n",
    "and use the same seed throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc35a3-11a9-4ad3-a282-20b067e1d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The sound of Brazilian samba drums with waves gently crashing in the background\"\n",
    "\n",
    "audio = pipe(prompt, audio_length_in_s=10, generator=generator).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eabdc7-9296-4bb9-99d7-0d0513d1608d",
   "metadata": {},
   "source": [
    "Cool! That run took about 13 seconds to generate. Let's have a listen to the output audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edbe04-93d9-4b38-875b-1fca214a23e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358757a-51c9-4401-8ab7-49d22d28878e",
   "metadata": {},
   "source": [
    "Sounds much like our text prompt! The quality is good, but still has artefacts of background noise. We can provide the pipeline with a [*negative prompt*](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.negative_prompt) to discourage the pipeline from generating certain features. In this case, we'll pass a negative prompt that discourages the model from generating low quality audio in the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c0f7ba-f2f6-4d19-83b7-260dab90fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prompt = \"Low quality, average quality.\"\n",
    "\n",
    "audio = pipe(prompt, negative_prompt=negative_prompt, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6159ed-a121-4dcc-8e19-7b507bc508cf",
   "metadata": {},
   "source": [
    "The inference time is un-changed when using a negative prompt; we simply replace the unconditional input to the LDM with the negative input. That means any gains we get in audio quality we get for free!\n",
    "\n",
    "Let's take a listen to the resulting audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61036f7e-d56d-4282-b09a-5dcb78a883bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf784d-8f45-46bc-a710-6ede1981b0a4",
   "metadata": {},
   "source": [
    "There's definitely an improvement in the overall audio quality - there are less noise artefacts and the audio generally sounds sharper. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea1a5e-2d8b-4bd2-ad1a-cd3366aef507",
   "metadata": {},
   "source": [
    "## Optimisation 1: Flash Attention\n",
    "\n",
    "PyTorch 2.0 and upwards includes an optimised and memory-efficient implementation of the attention operation through the [`torch.nn.functional.scaled_dot_product_attention`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention) (SDPA) function. This function automatically applies several in-built optimisations depending on the inputs. Overall, the SDPA function gives similar behaviour to the [flash attention](https://arxiv.org/abs/2205.14135) implementation.\n",
    "\n",
    "These optimisations will be enabled by default in Diffusers if PyTorch 2.0 is installed and if `torch.nn.functional.scaled_dot_product_attention` is available. To use it, just install torch 2.0 as suggested above and simply use the pipeline as is!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c49f74-ad41-44cb-9a59-3866b54d87fe",
   "metadata": {},
   "source": [
    "## Optimisation 2: Half-Precision\n",
    "\n",
    "By default, the `AudioLDM2Pipeline` loads the model weights in float32 (full) precision. All the model computations are also performed in float32 precision. For inference, we can safely convert the model weights and computations to float16 (half) precision, which will give us an improvement to inference time and GPU memory, with an impercivable change to generation quality.\n",
    "\n",
    "We can load the weights in float16 precision by passing the [`torch_dtype`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/overview#diffusers.DiffusionPipeline.from_pretrained.torch_dtype) argument to `.from_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fe401-ea99-4710-ac6b-9e6129aa088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "pipe.to(\"cuda\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112a106-226b-456a-8ab6-3b491743cc5b",
   "metadata": {},
   "source": [
    "Let's run generation in float16 precision and listen to the audio outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab3d39-c1a9-44d5-8ebd-c06cadd16f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]\n",
    "\n",
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135dbbd3-4678-4c9e-b6a8-d624ae20bbf0",
   "metadata": {},
   "source": [
    "The audio quality is largely un-changed from the full precision generation, with an inference speed-up of about 2 seconds. In our experience, we've not seen any significant audio degradation using `diffusers` pipelines with float16 precision, thus we recommend using float16 precision by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313dfb-666f-4c3b-a675-64f5beac6edd",
   "metadata": {},
   "source": [
    "## Optimisation 3: Torch Compile\n",
    "\n",
    "To get an additional speed-up, we can use the new `torch.compile` feature. Since the UNet of the pipeline is usually the most computationally expensive, \n",
    "we wrap the unet with `torch.compile`, leaving the rest of the sub-models (text encoders and VAE) as is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6890c45a-ab78-4908-9d95-d21d95ae58ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcbe36d-8776-4b20-bb8c-09d6143ce740",
   "metadata": {},
   "source": [
    "After wrapping the UNet with `torch.compile` the first inference step we run is typically going to be slow, due to the overhead of compiling the forward pass of the UNet. Let's run the pipeline forward with the compilation step get this longer run out of the way. Note that the first inference step might take up to 2 minutes to compile, so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ded80-f5b8-4abb-bf32-108dcc084b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873f0f8-3363-4838-9f33-f3c989cb8935",
   "metadata": {},
   "source": [
    "Great! Now that the UNet is compiled, we can now run the full diffusion process and reap the benefits of faster inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871551eb-544e-4e76-842f-bb98869af3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a7a8c-acd8-4baf-b11a-05a88f90894a",
   "metadata": {},
   "source": [
    "Only 4 seconds to generate! In practice, you will only have to compile the UNet once, and then get faster inference for all successive generations. This means that the time taken to compile the model is amortised by the gains in subsequent inference time. For more information and options regarding `torch.compile`, refer to the [torch compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b081d06-344d-4f34-9e31-6939d2938c2c",
   "metadata": {},
   "source": [
    "## Optimisation 4: Scheduler\n",
    "\n",
    "Another option is to reduce the number of inference steps. Choosing a more efficient scheduler can help decrease the number of steps without sacrificing the output audio quality. You can find which schedulers are compatible with the `AudioLDM2Pipeline` by calling the [`schedulers.compatibles`](https://huggingface.co/docs/diffusers/v0.20.0/en/api/schedulers/overview#diffusers.SchedulerMixin) attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d647f12d-cb41-4b92-b089-1b4cdcf6f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.scheduler.compatibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06679e-a299-4e28-9537-09fb9d0e5516",
   "metadata": {},
   "source": [
    "Alright! We've got a long list of schedulers to choose from 📝. By default, AudioLDM 2 uses the [`DDIMScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/ddim), and requires 200 inference steps to get good quality audio generations. However, more performant schedulers, like [`DPMSolverMultistepScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler), require only **20-25 inference steps** to achieve similar results.\n",
    "\n",
    "Let's see how we can switch the AudioLDM 2 scheduler from DDIM to DPM Multistep. We'll use the [`ConfigMixin.from_config()`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) method to load a [`DPMSolverMultistepScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler) from the configuration of our original [`DDIMScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/ddim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403263b1-e49f-4972-9c54-69428b8818c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f323368-1dc2-4405-aa0b-84e79e11041a",
   "metadata": {},
   "source": [
    "Let's set the number of inference steps to 20 and re-run the generation. We'll have to re-compile the UNet, so we can leave it to run for one generation process. Again, this will take up to 2 minutes to compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06488ec2-bd40-44dd-9547-08c4533cf019",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=20, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8432f8f-c0a3-4d64-ad92-0b24e9baf6e3",
   "metadata": {},
   "source": [
    "Now we're ready to benchmark! We can re-run generation again using the compiled UNet with the DPM Multistep scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dfc1ee-d004-421a-89ba-67a2496f7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=20, audio_length_in_s=10, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7d212-7f98-4652-b381-6f28e81f3836",
   "metadata": {},
   "source": [
    "That took less than **1 second** to generate the audio! Let's have a listen to the resulting generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b691ece4-96dc-4909-9495-6b49b9acb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c8986-4747-4b4b-ba38-afdaa59caee5",
   "metadata": {},
   "source": [
    "More or less the same as our original audio sample, but only a fraction of the generation time! 🧨 Diffusers pipelines are designed to be *composable*, allowing you two swap out schedulers and other components for more performant counterparts with ease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1dd05-1804-4f98-bbad-e495c09b4eec",
   "metadata": {},
   "source": [
    "## What about memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fb7b3-98ef-4fad-a012-021c2fdd6b4e",
   "metadata": {},
   "source": [
    "The length of the audio sample we want to generate dictates the *width* of the latent variables we de-noise in the LDM. Since the memory of the cross-attention layers in the UNet scales with sequence length (width) squared, generating very long audio samples might lead to out-of-memory errors.\n",
    "\n",
    "We've already mentioned that loading the model in float16 half precision gives strong memory savings. Using PyTorch 2.0 SDPA also gives a memory improvement, but this might not be suffienct for extremely large sequence lengths.\n",
    "\n",
    "Let's try generating an audio sample five minutes (300 seconds) in duration. We'll also generate three candidate audios by setting [`num_waveforms_per_prompt`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.num_waveforms_per_prompt)`=3`. Once [`num_waveforms_per_prompt`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.__call__.num_waveforms_per_prompt)`>1`, automatic scoring is performed between the generated audios and the text prompt: the audios and text prompts are embedded in the CLAP audio-text embedding space, and then ranked based on their cosine similarity scores.\n",
    "\n",
    "Since we've changed the width of the latent variables in the UNet, we'll have to perform another torch compilation step with the new latent variable shapes. In the interest of time, we'll re-load the pipeline without torch compile, such that we're not hit with a lengthy compilation step first up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73465c-6bb8-4d8f-9f32-abc8e0f0ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AudioLDM2Pipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "audio = pipe(prompt, negative_prompt=negative_prompt, num_waveforms_per_prompt=3, num_inference_steps=20, audio_length_in_s=300, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835eac3-e4a2-45c6-8830-db5905873705",
   "metadata": {},
   "source": [
    "Unless you have a GPU with high RAM, the code above probably returned an OOM error. While the AudioLDM 2 pipeline involves sevaral components, only the model being used has to be on the GPU at any one time. The remainder of the modules can be offloaded to the CPU. This technique, called *CPU offload*, can reduce memory usage, with a very low penalty to inference time.\n",
    "\n",
    "We can enable CPU offload on our pipeline with the function [enable_model_cpu_offload()](https://huggingface.co/docs/diffusers/main/en/api/pipelines/audioldm2#diffusers.AudioLDM2Pipeline.enable_model_cpu_offload):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaff790-3384-4f34-9dd7-b2059ef43879",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60be5b-d5f8-4601-b893-71ebff9e72ef",
   "metadata": {},
   "source": [
    "Running generation with CPU offload is then the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35b047-4a00-409a-8c15-898223163060",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = pipe(prompt, negative_prompt=negative_prompt, num_waveforms_per_prompt=3, num_inference_steps=20, audio_length_in_s=300, generator=generator.manual_seed(0)).audios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d86b24-8887-4c61-aebb-a883d249dfdd",
   "metadata": {},
   "source": [
    "And with that, we can generate a five minute audio sample in one call to the pipeline! Using the large AudioLDM 2 checkpoint will result in higher overall memory usage than the base checkpoint, since the UNet is over twice the size (750M parameters compared to 350M), so this memory saving trick is particularly beneficial here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec16b5ee-9226-41b3-8e2b-64e2ed7f3f11",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d30c56d-84d7-43d0-8c6e-c4809d03703c",
   "metadata": {},
   "source": [
    "In this Colab, we showcased four optimisation methods that are available out of the box with 🧨 Diffusers, taking the generation time of AudioLDM 2 from 14 seconds down to less than 1 second. We also highlighted how to employ memory saving tricks, such as half-precision and CPU offload, to reduce peak memory usage for long audio samples or large checkpoint sizes.\n",
    "\n",
    "Notebook by [Sanchit Gandhi](https://huggingface.co/sanchit-gandhi). Spectrogram image source: [Getting to Know the Mel Spectrogram](https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0). Waveform image source: [Aalto Speech Processing](https://speechprocessingbook.aalto.fi/Representations/Waveform.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
