{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsbQp3z/WHJTwkUQXmwJM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/jax_gemma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Gemma on Colab TPU\n",
        "\n",
        "Gemma is a family language models released by Google DeepMind in the paper [Gemma: Open Models Based on Gemini Research and Technology](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf). Gemma leverages the same research and technology used to create the Gemini models. It was trained using a dataset consisting of 6-trillion tokens, formed from web documents, code and mathematics. The result is a series of state-of-the-art models at the 2B and 7B scale, all of which are open-sourced and permissively licensed.\n",
        "\n",
        "JAX is a Python library for high-performance machine-learning research. It uses the XLA (accelerator linear algebra) compiler to fuse sequences of operations together and run them at once. In the context of machine-learning, this allows JAX to execute blocks of modelling code very efficiently on modern hardware accelerators, like GPUs and TPUs, making it appealing for large-scale research and applications. Gemma itself was trained using JAX on TPU v5e cores.\n",
        "\n",
        "Gemma has support in the ðŸ¤— Transformers library from day-1, in both PyTorch and JAX. In this Google Colab, we'll showcase how to use the Gemma 2B model in JAX, leveraging the power of TPU v2 cores to run batched generation with a throughput of 475 tokens per second. For details on using Gemma in PyTorch on GPU, refer to the corresponding [documentation](https://huggingface.co/google/gemma-2b#usage).\n"
      ],
      "metadata": {
        "id": "9lx4FYm7n5Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to a TPU\n",
        "\n",
        "First, we need to register our Hugging Face Hub token with our Google Colab runtime. Since the Gemma model is gated, our token will be checked when the model is downloaded to ensure we have accepted the terms-of-use.\n",
        "\n",
        "To register your token, click the key symbol ðŸ”‘ in the left-hand pane of the screen. Name the secret `HF_TOKEN`, and copy a token from your Hugging Face Hub account: https://huggingface.co/settings/tokens. Your token should now be registered, allowing you to access the Gemma weights to this Colab session.\n",
        "\n",
        "Next, we can connect to a TPU. Google Colab offers TPU v2's on its free tier, which we'll make use of for this notebook. If you have access to later generations of TPU, such as v3, v4 or v5e machines, you can run the same notebook and achieve significantly faster generation speeds.\n",
        "\n",
        "To connect to a TPU v2, click on the button `Connect TPU` in the top right-hand corner of the screen. Once connected to a TPU, we need to initialise it with the `setup_tpu` method:"
      ],
      "metadata": {
        "id": "pcAAkx8LKaHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.tools.colab_tpu\n",
        "\n",
        "jax.tools.colab_tpu.setup_tpu()"
      ],
      "metadata": {
        "id": "RoCC9NtWpLVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the following code cell to see the TPU devices we have available:"
      ],
      "metadata": {
        "id": "7tsi-noROsg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "\n",
        "jax.devices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XWpUuCgo7kd",
        "outputId": "206c2638-0c11-4d11-a06b-681358a9d04e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We have a TPU device with 8 chips. In this notebook, we'll leverage *data parallelism* across these 8 chips, by sending 1/8 of our batch to each chip and generating the LLM outputs in parallel. TPUs shine at higher batch-sizes, where computations can easily be distributed across devices.\n",
        "\n",
        "JAX comes pre-installed on Colab TPUs at the correct version, so there's no need to re-install it. However, we do need to updgrade Transformers to the latest version, since the Gemma model is only included in the latest library release:"
      ],
      "metadata": {
        "id": "dDjG76wZOvCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet transformers"
      ],
      "metadata": {
        "id": "r5mij3MdmtSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model\n",
        "\n",
        "The Gemma model can be loaded using the familiar [`from_pretrained`](https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained) method in Transformers. This method downloads the model weights from the Hugging Face Hub the first time it is called, and subsequently intialises the Gemma model using these weights.\n",
        "\n",
        "We'll load the weights for the base 2B model by specifying the corresponding model-id on the Hugging Face Hub ([\"google/gemma-2b\"](https://huggingface.co/google/gemma-2b)). You should ensure you have accepted the model terms-of-use before downloading the model. To do so, simply head to the model repository [here](https://huggingface.co/google/gemma-2b) and fill out the required fields.\n",
        "\n",
        "We'll set the data-type (dtype) of the computation to bfloat16, which is faster than float32 while achieving similar accuracy. Finally, we'll set the flag `_do_init=False`, which improves the loading of large models in Transformers by skipping initialising a dummy set of parameters that are subsequently overriden by the pre-trained ones."
      ],
      "metadata": {
        "id": "Rb2VBFGJQ1yx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjQ4e57vmWCP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "from flax import jax_utils\n",
        "from flax.training.common_utils import shard\n",
        "\n",
        "from transformers import FlaxGemmaForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, params = FlaxGemmaForCausalLM.from_pretrained(\"google/gemma-2b\", revision=\"flax\", _do_init=False, dtype=jnp.bfloat16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keuwma2emdRM",
        "outputId": "1a824d2e-0b35-4260-b2a3-060ba6265e22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some of the weights of FlaxGemmaForCausalLM were initialized in bfloat16 precision from the model checkpoint at google/gemma-2b:\n",
            "[('model', 'embed_tokens', 'embedding'), ('model', 'layers', '0', 'input_layernorm', 'weight'), ('model', 'layers', '0', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '0', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '0', 'post_attention_layernorm', 'weight'), ('model', 'layers', '0', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '0', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '1', 'input_layernorm', 'weight'), ('model', 'layers', '1', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '1', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '1', 'post_attention_layernorm', 'weight'), ('model', 'layers', '1', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '1', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '10', 'input_layernorm', 'weight'), ('model', 'layers', '10', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '10', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '10', 'post_attention_layernorm', 'weight'), ('model', 'layers', '10', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '10', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '11', 'input_layernorm', 'weight'), ('model', 'layers', '11', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '11', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '11', 'post_attention_layernorm', 'weight'), ('model', 'layers', '11', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '11', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '12', 'input_layernorm', 'weight'), ('model', 'layers', '12', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '12', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '12', 'post_attention_layernorm', 'weight'), ('model', 'layers', '12', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '12', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '13', 'input_layernorm', 'weight'), ('model', 'layers', '13', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '13', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '13', 'post_attention_layernorm', 'weight'), ('model', 'layers', '13', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '13', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '14', 'input_layernorm', 'weight'), ('model', 'layers', '14', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '14', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '14', 'post_attention_layernorm', 'weight'), ('model', 'layers', '14', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '14', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '15', 'input_layernorm', 'weight'), ('model', 'layers', '15', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '15', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '15', 'post_attention_layernorm', 'weight'), ('model', 'layers', '15', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '15', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '16', 'input_layernorm', 'weight'), ('model', 'layers', '16', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '16', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '16', 'post_attention_layernorm', 'weight'), ('model', 'layers', '16', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '16', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '17', 'input_layernorm', 'weight'), ('model', 'layers', '17', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '17', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '17', 'post_attention_layernorm', 'weight'), ('model', 'layers', '17', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '17', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '2', 'input_layernorm', 'weight'), ('model', 'layers', '2', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '2', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '2', 'post_attention_layernorm', 'weight'), ('model', 'layers', '2', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '2', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '3', 'input_layernorm', 'weight'), ('model', 'layers', '3', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '3', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '3', 'post_attention_layernorm', 'weight'), ('model', 'layers', '3', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '3', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '4', 'input_layernorm', 'weight'), ('model', 'layers', '4', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '4', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '4', 'post_attention_layernorm', 'weight'), ('model', 'layers', '4', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '4', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '5', 'input_layernorm', 'weight'), ('model', 'layers', '5', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '5', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '5', 'post_attention_layernorm', 'weight'), ('model', 'layers', '5', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '5', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '6', 'input_layernorm', 'weight'), ('model', 'layers', '6', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '6', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '6', 'post_attention_layernorm', 'weight'), ('model', 'layers', '6', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '6', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '7', 'input_layernorm', 'weight'), ('model', 'layers', '7', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '7', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '7', 'post_attention_layernorm', 'weight'), ('model', 'layers', '7', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '7', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '8', 'input_layernorm', 'weight'), ('model', 'layers', '8', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '8', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '8', 'post_attention_layernorm', 'weight'), ('model', 'layers', '8', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '8', 'self_attn', 'v_proj', 'kernel'), ('model', 'layers', '9', 'input_layernorm', 'weight'), ('model', 'layers', '9', 'mlp', 'down_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'gate_proj', 'kernel'), ('model', 'layers', '9', 'mlp', 'up_proj', 'kernel'), ('model', 'layers', '9', 'post_attention_layernorm', 'weight'), ('model', 'layers', '9', 'self_attn', 'k_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'o_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'q_proj', 'kernel'), ('model', 'layers', '9', 'self_attn', 'v_proj', 'kernel'), ('model', 'norm', 'weight')]\n",
            "You should probably UPCAST the model weights to float32 if this was not intended. See [`~FlaxPreTrainedModel.to_fp32`] for further information on how to do this.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you're coming from PyTorch, the only major difference in API is how the model and parameters are handled. PyTorch is a _stateful_ framework, in which the weights are stored within the model instance. In JAX, most transformations (notably `jax.jit`) require functions that are _stateless_, meaning that they have no side effects (see [Stateful Computations](https://jax.readthedocs.io/en/latest/jax-101/07-state.html) in JAX). Since Flax models are designed to work well with JAX transformations, they too are stateless. This means that the model weights are stored **outside** of the model definition, and need to be passed as an input during inference.\n",
        "\n",
        "We see a warning that the model parameters were loaded in bfloat16 precision - this is fine since we also want to keep the parameters in bfloat16 for inference."
      ],
      "metadata": {
        "id": "RHSJh5sgTSZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The corresponding tokenizer can now be loaded using a similar API:"
      ],
      "metadata": {
        "id": "GQMjMGOLeHF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
      ],
      "metadata": {
        "id": "SOgY8-DroH4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Inputs\n",
        "\n",
        "Next, we'll define our text inputs. Since we have 8 TPU cores over which we want to perform data parallelism, we need our batch size to be a multiple of 8. This is to ensure that each TPU core receives the same amount of data (`bsz / 8` samples).\n",
        "\n",
        "In this example, we'll define a single prompt and copy it 8 times, giving a total batch size of 8. The \"per-device\" batch size is then 1/8 of this, or 1. In practice, this is not very useful or realistic, since each of our parallel computations will be processing the same input. However, you're free to extend this to use more realistic prompts than the one given below. Just ensure that the resulting batch size is a multiple of 8."
      ],
      "metadata": {
        "id": "BVz3ylFwepfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 8 * [\"The capital of France is\"]"
      ],
      "metadata": {
        "id": "-XciqMHmsQqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pre-process our input text to token ids using the tokenizer. TPUs expect inputs of static shape, so we'll define our maximum prompt length to be 64, and always pad our inputs to this sequence length:"
      ],
      "metadata": {
        "id": "QuePjDn6sO3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 64\n",
        "\n",
        "inputs = tokenizer(\n",
        "    input_text,\n",
        "    padding=\"max_length\",\n",
        "    max_length=max_input_length,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors=\"np\",\n",
        ")"
      ],
      "metadata": {
        "id": "X9a5KeFUoE6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to copy the model parameters to each TPU core. Each core will hold it's own copy of the parameters, such that it can run a model generation in parallel with the others. Copying the parameters across devices is achieved simply with the [`replicate`](https://flax.readthedocs.io/en/latest/api_reference/flax.jax_utils.html#flax.jax_utils.replicate) method from Flax."
      ],
      "metadata": {
        "id": "eW3iFWNeUAxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = jax_utils.replicate(params)"
      ],
      "metadata": {
        "id": "aMA1qL-5ofwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we need to split (or shard) our inputs across TPU cores. Given input ids of shape `(bsz, seq_len)`, we'll shard them to `(num_devices, bsz / num_devices, seq_len)`. In doing so, each device can receive a batch of shape `(bsz / num_devices, seq_len)`, in order to leverage data parallelism across TPU cores. Sharding our inputs is achieved with the Flax helper function [`shard`](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html#flax.training.common_utils.shard):"
      ],
      "metadata": {
        "id": "Xaygh3WvUh0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = shard(inputs.data)"
      ],
      "metadata": {
        "id": "yEUlaMepUi1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "We can now define our data-parallel method for inference. The Transformers [`generate`](https://huggingface.co/docs/transformers/v4.38.1/en/main_classes/text_generation#transformers.FlaxGenerationMixin.generate) method provides functionality for auto-regressive generation with batching, sampling, beam-search, etc. To reap the benefits of JAX, we'll compile the generate method end-to-end, such that the operations are fused into XLA-optimised kernels and executed efficiently on our hardware accelerator.\n",
        "\n",
        "To achieve this, we'll wrap the `generate` method with the [`jax.pmap`](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html) transformation. The `jax.pmap` transformation compiles the `generate` method with XLA, and prepares a function that can be executed in parallel across TPU devices."
      ],
      "metadata": {
        "id": "Qfb-MXRPUZY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(inputs, params, max_new_tokens):\n",
        "    generated_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        params=params,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "    )\n",
        "    return generated_ids.sequences\n",
        "\n",
        "p_generate = jax.pmap(\n",
        "    generate, \"inputs\", in_axes=(0, 0, None,), out_axes=0, static_broadcasted_argnums=(2,)\n",
        ")"
      ],
      "metadata": {
        "id": "tOE2UjXqUZvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `in_axes` argument to `jax.pmap` defines which axis of the positional arguments to parallelise over. The `inputs` and `params` are parallised over their first axis, which we denote by `0`. The number of generated tokens is a integer, and is thus not parallelised over, which we denote by `None`.\n",
        "\n",
        "Similarly, the `out_axes` argument defines which axis of the outputs are parallelised over, which in this case is the first axis.\n",
        "\n",
        "The number of maximum generated tokens `max_new_tokens` undergoes control flow in `generate`. However, JIT requires concrete values to trace out the transformed function (see [Control Flow](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#control-flow) in JAX). Thus, we specify it as a static argument, meaning it is assigned as a concrete value in the control flow. This enables JIT to trace out the function for a specific value of `max_new_tokens`. The caveat is that each value of `max_new_tokens` requires it's own compilation, in-order to trace out the respective branch.\n",
        "\n",
        "To avoid re-compiling the generate function for different values of `max_new_tokens`, we'll define it as a global variable here, and pass it to the generate function each time:"
      ],
      "metadata": {
        "id": "cepVL9tne2jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 128"
      ],
      "metadata": {
        "id": "uVf9xbJBoYUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compile our parallel generate function. This done automatically the first time the function is run and will take some time to complete (typically around 2-3 minutes). A good time to read the JAX [Quick Start Guide](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) if you haven't already!"
      ],
      "metadata": {
        "id": "eTC7I3bmWXgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_ = p_generate(inputs, params, max_new_tokens)"
      ],
      "metadata": {
        "id": "Ws1XC2PGHBYu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "330dd2bb-657b-406b-88b8-b525b1d42d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n",
            "/usr/local/lib/python3.10/dist-packages/jax/_src/ops/scatter.py:87: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=bfloat16 to dtype=float32. In future JAX releases this will result in an error.\n",
            "  warnings.warn(\"scatter inputs have incompatible types: cannot safely cast \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the function is compiled, we can run it again much faster using the optimised kernels:"
      ],
      "metadata": {
        "id": "XYxnfADQWqxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "generated_ids = p_generate(inputs, params, max_new_tokens)\n",
        "runtime = time.time() - start"
      ],
      "metadata": {
        "id": "g1JbDhJQmh_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generate function returns a batch of generated token ids. To convert these to generated text, we can decode them using the tokenizer:"
      ],
      "metadata": {
        "id": "qDOUrhKMW2cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = jax.device_get(generated_ids.reshape(-1, generated_ids.shape[-1]))\n",
        "pred_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "z-bUqMoJs6t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So how fast was generation? Let's compute the number of tokens generated per-second (tok/s):"
      ],
      "metadata": {
        "id": "MGg9UGRRXBu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tok_per_s(input_ids, generated_ids, runtime):\n",
        "    total_inputs = np.prod(input_ids.shape)\n",
        "    total_outputs = np.prod(generated_ids.shape)\n",
        "    tokens_generated = total_outputs - total_inputs\n",
        "    tokens_per_s = tokens_generated / runtime\n",
        "    return tokens_per_s\n",
        "\n",
        "tok_per_s = compute_tok_per_s(inputs[\"input_ids\"], generated_ids, runtime)"
      ],
      "metadata": {
        "id": "jjbaict3IRSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then print our runtime, tokens per second, and generated text to the console:"
      ],
      "metadata": {
        "id": "4MWwzPuCXwxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Runtime with pmap: {runtime}\")\n",
        "print(f\"Tokens per second: {tok_per_s}\")\n",
        "print(pred_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE6EzHx6XsZp",
        "outputId": "317b6cde-030e-46e0-ad07-47d41005a03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime with pmap: 1.612574815750122\n",
            "Tokens per second: 635.0092969321647\n",
            "The capital of France is Paris, the cultural capital of Europe and a favourite stopover for numerous visitors from everywhere in the world. Located in the heart of France, it is the city where art, fashion, music and gastronomy are so highly esteemed. But above all, Paris is a place with an inimitable ambiance, a place where the magic and charm of the past meet the reality of the most contemporary developments. A city that will charm you with its diversity and elegance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Around 635 tokens per second! For context, the same batch of inputs takes 8.4 seconds to generate on an A100 GPU in PyTorch at a rate of 122 tokens per second.\n",
        "\n",
        "To see how compiled inference holds for other inputs, we can update our input text and re-run generation again:"
      ],
      "metadata": {
        "id": "u7AxbY5VY8bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = 4 * [\"A recipe for coconut pasta:\", \"In cricket, the cover drive\"]\n",
        "inputs = tokenizer(input_text, padding=\"max_length\", max_length=max_input_length, return_attention_mask=True, return_tensors=\"np\")\n",
        "inputs = shard(inputs.data)\n",
        "\n",
        "start = time.time()\n",
        "generated_ids = p_generate(inputs, params, max_new_tokens)\n",
        "runtime = time.time() - start\n",
        "\n",
        "generated_ids = jax.device_get(generated_ids.reshape(-1, generated_ids.shape[-1]))\n",
        "pred_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "4A53H_GsZENs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that a similar generation speed applies for these new inputs as well:"
      ],
      "metadata": {
        "id": "ypKqjP-la492"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok_per_s = compute_tok_per_s(inputs[\"input_ids\"], generated_ids, runtime)\n",
        "\n",
        "print(f\"Runtime with pmap: {runtime}\")\n",
        "print(f\"Tokens per second: {tok_per_s}\")\n",
        "print(pred_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjvMk1KhZRQo",
        "outputId": "bde7c3b0-09a4-4087-c872-f62cd20d40cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime with pmap: 2.1800482273101807\n",
            "Tokens per second: 469.71437932978523\n",
            "A recipe for coconut pasta: Mix together 1 cup of coconut milk, 2 cups of fresh or frozen spinach, 1 1/2 cups of kale, 1 1/2 cups of shredded carrots, 2 tablespoons of olive oil, 1/2 teaspoon of salt, and 1/2 teaspoon of pepper. Combine the ingredients in a blender until the mixture is smooth and creamy. Serve the mixture over cooked pasta to create a delicious and healthy dish.\n",
            "\n",
            "<h2>How do you make coconut noodles?</h2>\n",
            "\n",
            "1. Heat 1 tablespoon of olive oil in a wok or large skillet over medium-high heat. Add 2 garlic cloves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this Colab, we introduced the Gemma model from Google DeepMind and showcased how to run inference in JAX using the Transformers library. We compiled the generate call using `jax.pmap`, giving XLA-optimised kernels for TPU. The result was generation speeds of 475 tokens per second, around 4x faster than on equivalent GPU hardware.\n",
        "\n",
        "It's worth noting that the Cloud TPU v2s used in the Google Colab free-tier are now 3 generations old. Running the same code on the latest TPU hardware (e.g. TPU v4 or v5e) gives a significant performance gain compared to older generation v2s."
      ],
      "metadata": {
        "id": "jL3Fhra2rTpo"
      }
    }
  ]
}