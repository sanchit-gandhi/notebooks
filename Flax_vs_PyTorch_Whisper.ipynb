{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/Flax_vs_PyTorch_Whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339d26f0-cb29-431c-912a-3e8c72307f58",
      "metadata": {
        "id": "339d26f0-cb29-431c-912a-3e8c72307f58"
      },
      "source": [
        "# Flax Whisper vs PyTorch Whisper\n",
        "\n",
        "In this notebook, we demonstrate how you can run inference with Whisper up to **10x faster** in Flax than PyTorch on a Colab GPU with largely the same code."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup the environment\n",
        "\n",
        "First of all, let's try to secure a decent GPU for our Colab! To get a GPU, click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _None_ to _GPU_.\n",
        "\n",
        "We can verify that we've been assigned a GPU and view its specifications:"
      ],
      "metadata": {
        "id": "fbnGKNLC9Rwq"
      },
      "id": "fbnGKNLC9Rwq"
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvS_nAJb9nen",
        "outputId": "ca4374fe-1813-484a-adef-eca00d948603"
      },
      "id": "lvS_nAJb9nen",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar  2 11:01:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    27W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we install [Flax](https://flax.readthedocs.io/en/latest/), [Datasets](https://github.com/huggingface/datasets) and [Transformers](https://github.com/huggingface/transformers) from main:"
      ],
      "metadata": {
        "id": "SPhCX1_h93kl"
      },
      "id": "SPhCX1_h93kl"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet flax datasets>=2.6.1 git+https://github.com/huggingface/transformers "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt2PMOU09ylF",
        "outputId": "eb6c153e-83a4-45ca-cf89-0f8a6af105c4"
      },
      "id": "Vt2PMOU09ylF",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, import all the required packages:"
      ],
      "metadata": {
        "id": "gkZN_Jzm-Ac6"
      },
      "id": "gkZN_Jzm-Ac6"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16e7928c-016c-4dba-9715-48c2843347d4",
      "metadata": {
        "id": "16e7928c-016c-4dba-9715-48c2843347d4"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import FlaxWhisperForConditionalGeneration, WhisperForConditionalGeneration, WhisperProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e9057f1-1032-49e4-af18-34be01d0ea36",
      "metadata": {
        "id": "8e9057f1-1032-49e4-af18-34be01d0ea36"
      },
      "source": [
        "## Benchmark Specifications\n",
        "\n",
        "We'll evalaute both the PyTorch and Flax Whisper models using the same checkpoint, defined below. You can change this to a checkpoint of your choice, including larger pre-trained checkpoints or fine-tuned variants. Refer to the [HF Hub](https://huggingface.co/models?sort=downloads&search=whisper) for a full list of checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "947fceb5-39d5-4f4e-89ca-b96f6b23c02a",
      "metadata": {
        "id": "947fceb5-39d5-4f4e-89ca-b96f6b23c02a"
      },
      "outputs": [],
      "source": [
        "model_id = \"openai/whisper-tiny.en\"  # change to a model checkpoint of your choice"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91fe709-498d-463f-9ca7-e90b1bdc4842",
      "metadata": {
        "id": "a91fe709-498d-463f-9ca7-e90b1bdc4842"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "We'll load a small dataset consisting of 73 samples from the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) validation-clean split. This amounts to ~9MB of data, so it's very lightweight and quick to download on device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "34ea4401-35c2-4646-bb6b-2732979e6778",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34ea4401-35c2-4646-bb6b-2732979e6778",
        "outputId": "b4229ee2-5f90-4938-eeb8-a57802d64e0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset librispeech_asr_dummy (/root/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b)\n"
          ]
        }
      ],
      "source": [
        "librispeech = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63937c49-0900-4f0d-8906-fce275ffa5bb",
      "metadata": {
        "id": "63937c49-0900-4f0d-8906-fce275ffa5bb"
      },
      "source": [
        "Let's preprocess the data by computing the log-mel features. These features will be the same for our PyTorch model and Flax one, so we only have to do this once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c0b85385-af4a-411f-8139-28865d92dff5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b85385-af4a-411f-8139-28865d92dff5",
        "outputId": "89cf9663-cca9-43b8-9542-a7458613a57f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/hf-internal-testing___librispeech_asr_dummy/clean/2.1.0/d3bc4c2bc2078fcde3ad0f0f635862e4c0fef78ba94c4a34c4c250a097af240b/cache-5568ca2364df5343.arrow\n"
          ]
        }
      ],
      "source": [
        "processor = WhisperProcessor.from_pretrained(model_id)\n",
        "\n",
        "def preprocess(batch):\n",
        "    batch[\"input_features\"] = processor(batch[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"np\").input_features[0]\n",
        "    return batch\n",
        "\n",
        "dataset_processed = librispeech.map(preprocess, remove_columns=librispeech.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90e5bd5-ddde-4cf0-a096-bd4c657410ee",
      "metadata": {
        "id": "c90e5bd5-ddde-4cf0-a096-bd4c657410ee"
      },
      "source": [
        "## PyTorch Benchmark\n",
        "\n",
        "First, let's load the PyTorch model and move it to the GPU. We'll follow the official inference recommendations and evaluate in half (float16) precision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4f0c01e5-eded-4dd4-8f12-ceb11937dd8b",
      "metadata": {
        "id": "4f0c01e5-eded-4dd4-8f12-ceb11937dd8b"
      },
      "outputs": [],
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
        "model.to(\"cuda\")\n",
        "model.eval()\n",
        "model.half();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0612c2-b278-4e5a-b5e6-4f99c0803911",
      "metadata": {
        "id": "fe0612c2-b278-4e5a-b5e6-4f99c0803911"
      },
      "source": [
        "Next, we define our dataloader. For this benchmark, we'll perform single-batch inference (`batch_size=1`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "51b2d899-dfc4-4e5d-9861-ff878995a440",
      "metadata": {
        "id": "51b2d899-dfc4-4e5d-9861-ff878995a440"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset_processed.with_format(\"torch\"), batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce8e01c-9d3e-468e-b7f5-995e3bb2a0a8",
      "metadata": {
        "id": "dce8e01c-9d3e-468e-b7f5-995e3bb2a0a8"
      },
      "source": [
        "Finally, we perform inference over our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "71ecb7ba-9de8-455c-b9b2-94ac95ca16fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ecb7ba-9de8-455c-b9b2-94ac95ca16fe",
        "outputId": "bcf5f508-3609-4444-ff50-a4650d7f637b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 73/73 [00:20<00:00,  3.60it/s]\n"
          ]
        }
      ],
      "source": [
        "for batch in tqdm(dataloader):\n",
        "    input_features = batch[\"input_features\"].to(\"cuda\").half()\n",
        "    pred_ids = model.generate(input_features, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c88fc78-dec2-42c7-8426-e5701e1a463e",
      "metadata": {
        "id": "6c88fc78-dec2-42c7-8426-e5701e1a463e"
      },
      "source": [
        "Depending on the GPU allocated to the Colab, you can expect inference to take ~20sec for 73 samples on a T4 or ~8sec on a V100."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe83e043-6beb-460e-a4d9-3c143d9fbeae",
      "metadata": {
        "id": "fe83e043-6beb-460e-a4d9-3c143d9fbeae"
      },
      "source": [
        "## Flax Benchmark\n",
        "\n",
        "We perform the parallel steps to the PyTorch benchmark, this time in Flax. First, we load the model. JAX automatically places the model onto the accelerator device, we just need to specifiy the `dtype` as fp16. We set the flag `from_pt=True` to load and convert PyTorch weights if available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "00852ece-6251-4c6f-bbc2-b74ecf5744e1",
      "metadata": {
        "id": "00852ece-6251-4c6f-bbc2-b74ecf5744e1"
      },
      "outputs": [],
      "source": [
        "model = FlaxWhisperForConditionalGeneration.from_pretrained(model_id, dtype=jnp.float16, from_pt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d794ae4-86b4-4643-acfa-8e0d05e1de89",
      "metadata": {
        "id": "6d794ae4-86b4-4643-acfa-8e0d05e1de89"
      },
      "source": [
        "Again, we define our dataloader for single-batch inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8383de49-b11f-4e78-99f0-f13c3185f33c",
      "metadata": {
        "id": "8383de49-b11f-4e78-99f0-f13c3185f33c"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset_processed.with_format(\"numpy\"), batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ef80bf0-f9ca-49b6-b85e-88cca5457d0e",
      "metadata": {
        "id": "7ef80bf0-f9ca-49b6-b85e-88cca5457d0e"
      },
      "source": [
        "We [JIT](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#just-in-time-compilation-with-jax) the generate function so that we can compile it and re-use the cached kernels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d40adaf-7a6f-472f-8cfb-37ae3c8fc7fb",
      "metadata": {
        "id": "7d40adaf-7a6f-472f-8cfb-37ae3c8fc7fb"
      },
      "outputs": [],
      "source": [
        "jit_generate = jax.jit(model.generate, static_argnames=[\"max_length\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed6ef29-9001-47d5-8fde-7ca5eba53c87",
      "metadata": {
        "id": "3ed6ef29-9001-47d5-8fde-7ca5eba53c87"
      },
      "source": [
        "We run the compile step on the first batch of data. We only have to do this compilation once. Afterwards, we can re-use the kernels for fast inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2a92626a-c090-400e-bec0-41895f8446a0",
      "metadata": {
        "id": "2a92626a-c090-400e-bec0-41895f8446a0"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "input_features = jnp.array(batch[\"input_features\"], dtype=jnp.float16)\n",
        "pred_ids = jit_generate(input_features, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fba4baa-c9fe-4467-8a1d-f7bb6ad5928a",
      "metadata": {
        "id": "8fba4baa-c9fe-4467-8a1d-f7bb6ad5928a"
      },
      "source": [
        "Finally, we perform inference over our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "883598af-9fc3-4cc7-b43a-da9aff6ebc60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "883598af-9fc3-4cc7-b43a-da9aff6ebc60",
        "outputId": "45a8a7b8-d47b-4550-fdf3-aacd8a9f01fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 73/73 [00:02<00:00, 29.56it/s]\n"
          ]
        }
      ],
      "source": [
        "for batch in tqdm(dataloader):\n",
        "    input_features = jnp.array(batch[\"input_features\"], dtype=jnp.float16)\n",
        "    pred_ids = jit_generate(input_features, max_length=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94940b14-7d91-4634-91bf-eb3617acb2ee",
      "metadata": {
        "id": "94940b14-7d91-4634-91bf-eb3617acb2ee"
      },
      "source": [
        "Depending on the GPU allocated to the Colab, you can expect inference to take ~2sec for 73 samples. This is a factor of **10x faster** than PyTorch on a T4⚡️"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}