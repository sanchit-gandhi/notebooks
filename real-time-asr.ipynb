{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc899f4d-5627-40ad-83c5-904da60e4c14",
   "metadata": {},
   "source": [
    "## Real-Time Speech Recognition with Distil-Whisper\n",
    "\n",
    "In this notebook, we'll showcase how to run streaming speech recognition using the [Distil-Whisper](https://huggingface.co/distil-whisper/distil-medium.en) model\n",
    "in the ðŸ¤— Transformers library. Streaming speech recognition works by constantly listening to the audio input, and continuously passing chunks of audio to the \n",
    "transcription model for inference.\n",
    "\n",
    "The Distil-Whisper model is lightweight and fast enough to be run locally on CPU, so there's no need for a specific hardware accelerator like GPU. We'll show \n",
    "how the chunk length of the audio can be controlled to give a trade-off between latency and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc869f-649c-43e8-a4c2-6fa9ef51b078",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Ensure you have PyTorch installed according to the [official instructions](https://pytorch.org/get-started/locally/), and the latest version of the Transformers library:\n",
    "\n",
    "```bash\n",
    "pip install --upgrate transformers\n",
    "```\n",
    "\n",
    "The microphone input relies on the `ffmpeg` library. You can verify that you have `ffmpeg` installed by running the following cell from the command line:\n",
    "```bash\n",
    "ffmpeg -version\n",
    "```\n",
    "If the `ffmpeg` library is not present, you can install it from the [`ffmpeg` homepage](https://www.ffmpeg.org/download.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85347260-1ac8-4cef-b40d-ec5581c97206",
   "metadata": {},
   "source": [
    "### Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d78c9f-8837-4232-86a4-f8aa1c601000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "import torch\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b164482-d37b-48cc-8f12-9eb5fc83fb0e",
   "metadata": {},
   "source": [
    "We'll load the [`distil-medium.en`](distil-whisper/distil-medium.en) checkpoint, which is 6.8x faster than `large-v2` and performs to within 2% word error rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8062cf22-1e96-4b6a-8cd6-c52ded0cfc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\" \n",
    "\n",
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"distil-whisper/distil-medium.en\", device=device\n",
    ")\n",
    "\n",
    "transcriber.model.generation_config.language = None\n",
    "transcriber.model.generation_config.task = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5376bdd-cf01-45ee-a5b4-b6903ca6ffcd",
   "metadata": {},
   "source": [
    "Now we'll define our transcription function, which listens to a total of `chunk_length_s` seconds of audio data. This audio data is broken down into `stream_chunk_s` second \n",
    "segments. After each new segment, we forward the total audio data to the model for transcription:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4816574a-34fc-4ec6-ad25-98a290d5473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(chunk_length_s=10.0, stream_chunk_s=1.0):\n",
    "    sampling_rate = transcriber.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "    for item in transcriber(mic, generate_kwargs={\"max_new_tokens\": 128}):\n",
    "        sys.stdout.write(\"\\033[K\")\n",
    "        print(item[\"text\"], end=\"\\r\")\n",
    "        if not item[\"partial\"][0]:\n",
    "            break\n",
    "\n",
    "    return item[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b618e44-6c7d-4638-9318-14f35c89fca9",
   "metadata": {},
   "source": [
    "Using a shorter `stream_chunk_s` lends itself to more real-time speech recognition, since we divide our input audio into smaller chunks and transcribe them on the fly. However, this comes at the expense of poorer accuracy, since thereâ€™s less context for the model to infer from.\n",
    "\n",
    "Let's apply the transcription function to our model and generate some real-time transcription results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b035294-a2fb-4db6-949a-3b8bb4fa8485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start speaking...\n",
      "\u001b[K Hey, I'm running the Distill Whisper model in real time using the Transformers library with a streaming input and a chunk length of one second.\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hey, I'm running the Distill Whisper model in real time using the Transformers library with a streaming input and a chunk length of one second.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcribe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1fd16-3dda-4d7a-80cc-518a285dc902",
   "metadata": {},
   "source": [
    "Looks good! Try running the above with different values of `stream_chunk_s` to see how the latency impacts the accuracy. For simplicity, we terminated our microphone recording after the first `chunk_length_s` seconds (which is set to 10 seconds by default). However, you can experiment with using a [voice activity detection (VAD)](https://huggingface.co/models?pipeline_tag=voice-activity-detection&sort=trending) model to predict when the user has stopped speaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a649eef-43b5-43e5-92a1-d48655aa37d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
