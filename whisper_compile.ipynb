{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f15e7c-c070-4c66-b902-e3d44e9adc66",
   "metadata": {},
   "source": [
    "## Whisper: Torch Compile for 4x Faster Inference\n",
    "\n",
    "The Whisper model in Transformers is *overhead bound*: the bottle-neck in inference speed is the CPU not instructing the GPU fast enough for the GPU to be fully utilized. To address this, we need to provide the GPU with more operations at once. One way of achieving this is using [**torch compile**](https://pytorch.org/docs/stable/generated/torch.compile.html), a native PyTorch function for accelerating the inference speed of PyTorch models.\n",
    "\n",
    "Torch compile takes a large region of a PyTorch graph and captures it into a single compiled region. In doing so, we can reduce the GPU instructions to this single compiled region, thus reducing the CPU overhead. Furthermore, torch compile generates faster kernels for the operations, speeding up computations and ensuring they are *memory bound*.\n",
    "\n",
    "In this Colab, we'll see how torch compile can be enabled for the Whisper model with just two lines of code. We'll perform a benchmark that highlights the 4x inference speed improvements that torch compile provides to any Whisper model on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407f1bb-1a0e-411b-800a-98badb0118db",
   "metadata": {},
   "source": [
    "## Set-Up\n",
    "\n",
    "The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all we need to do is hit the button `Connect T4` in the top right-hand corner of the screen.\n",
    "\n",
    "Once we've done that, we can go ahead and install the necessary Python packages. We'll install [ü§ó Transformers](https://huggingface.co/docs/transformers/index) for loading and running the Whisper models, [ü§ó Datasets](https://huggingface.co/docs/datasets/index) for loading our benchmarking dataset, and [ü§ó Accelerate](https://huggingface.co/docs/accelerate/index) for fast model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715711f-5b46-4224-bbe6-9b04609be08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet pip\n",
    "!pip install --upgrade --quiet transformers datasets[audio] accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea97037-4943-412e-a35e-843854ca4e8d",
   "metadata": {},
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f37892f-a7c6-4584-8fb7-1cf36a1410d9",
   "metadata": {},
   "source": [
    "First, we'll load the Whisper model and its accompanying processor using the familiar ü§ó Transformers API.\n",
    "In this example, we'll load the pre-trained Whisper [medium.en](https://huggingface.co/openai/whisper-medium.en) model, but you're free\n",
    "to swap this for any one of the [10k Whisper checkpoints](https://huggingface.co/models?library=transformers&other=whisper&sort=trending) \n",
    "on the Hugging Face Hub. To reduce the loading time, we'll pass the [low_cpu_mem_usage](https://huggingface.co/docs/transformers/v4.43.4/en/big_models#accelerates-big-model-inference) flag to `.from_pretrained`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c401e4-af6a-4aa7-8ad2-784b7d5d6772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/miniconda3/envs/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium.en\", torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
    "model.to(device)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium.en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c643f7-7230-4f25-83fc-d91511ebf4ce",
   "metadata": {},
   "source": [
    "For benchmarking, we'll load a small dataset consisting of 73 samples from the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) validation-clean dataset. This amounts to ~9MB of data, so it's very lightweight and quick to download on-device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae66173-b17b-4e8d-a2c9-2e7a64437927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 73\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d122d-2308-4104-9cac-ced762f3d0b9",
   "metadata": {},
   "source": [
    "To ensure the sampling rate of the audios matches the sampling rate of our model, we'll re-sample the audios to the sampling rate expected by Whisper (16kHz).\n",
    "Note that the re-sampling is applied on-the-fly when the audios are loaded, with a no-op if the sampling rate already matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13acc61e-fd65-430b-a140-7417b82d97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "sampling_rate = processor.feature_extractor.sampling_rate\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1af99-e43e-48c8-8c84-2704b0072b69",
   "metadata": {},
   "source": [
    "We're ready to start benchmarking üìè The following cell iterates over samples in our dataset one-by-one (i.e. with a batch size of one). For each sample, we perform three stages of inference:\n",
    "1. Pre-processing the raw audio inputs to log-mel spectrograms\n",
    "2. Auto-regressively generating the text tokens, conditional on the spectrogram inputs\n",
    "3. Post-processing the generated tokens to text strings\n",
    "\n",
    "For the purposes of benchmarking, we'll time the generation step, which is the portion performed by the Whisper model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d264c9d9-b6f6-4724-9875-890803d0475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/73 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:37<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.18650460243225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "\n",
    "inference_time = 0.0\n",
    "model.generation_config.max_new_tokens = 128\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    # 1. Pre-process the audio inputs\n",
    "    input_features = processor(sample[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(device, dtype=torch_dtype)\n",
    "    \n",
    "    # 2. Auto-regressively generate text tokens\n",
    "    start = time.time()\n",
    "    pred_ids = model.generate(input_features)\n",
    "    inference_time += time.time() - start\n",
    "\n",
    "    # 3. Post-process tokens to text\n",
    "    pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "print(inference_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435d2193-211a-4abb-9b66-78682451f59f",
   "metadata": {},
   "source": [
    "We have a baseline of **35.2-seconds** for our un-compiled model. Let's now apply torch compile and re-measure performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a13fa-70fa-4a84-83cf-12cf60b868e0",
   "metadata": {},
   "source": [
    "## Enable torch compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbe412-d210-4aeb-9db7-28adc438166d",
   "metadata": {},
   "source": [
    "The first step in enabling compile is self-explanatory: we need to apply the `torch.compile` transformation to the model forward pass. We'll set the compile mode to `reduce-overhead`, which uses [CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/) to further reduce CPU overhead. We'll also set `fullgraph=True` to compile the entire model in one graph (i.e. with no graph breaks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358ea02a-143f-400b-b244-e13cebda3d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c3295b-d2d6-434a-b1bf-3129f88086e1",
   "metadata": {},
   "source": [
    "The second step involves setting the key-value (kv) cache. During decoding, the Whisper decoder computes the kv states for each new input token and saves them to be re-used at the next decoding step, forming a **kv-cache**. The default kv-cache implementation grows in length with each generated token, since we save a new set of kv states for each decoding step.\n",
    "\n",
    "While dynamic shapes are compatible with a subset of `torch.compile` optimizations, they limit the extent to which the CPU overhead can be reduced. Thus, we'll switch the kv cache to a **static** implementation, which pre-allocates the entire kv-cache size to the maximum value and masks out the un-used parts from the attention computation. In doing so, this kv-cache implementation is compatible with the `reduce-overhead` setting from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029ce12d-46b9-49bf-82e9-9e1fc7a8f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.cache_implementation = \"static\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e163dc-56c9-4a51-8ad7-48460b6f27b9",
   "metadata": {},
   "source": [
    "Since torch compile is a \"just in time\" (JIT) compilation, we need to perform a set of compilation steps to compile our model. Here, we'll perform three warm-up steps, generating to our maximum number of permitted tokens each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f21c6c6f-5f73-47cc-9951-22dab3b36ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [03:51<00:00, 77.24s/it]\n"
     ]
    }
   ],
   "source": [
    "max_new_tokens = model.generation_config.max_new_tokens\n",
    "\n",
    "for _ in tqdm(range(3)):\n",
    "    with sdpa_kernel(SDPBackend.MATH):\n",
    "        model.generate(input_features, min_new_tokens=max_new_tokens, max_new_tokens=max_new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970d2613-c399-4a36-bace-76c5f65cb690",
   "metadata": {},
   "source": [
    "**Note:** this code-cell may take several minutes to run, particularly the first time it is called. To reduce the compilation time of subsequent runs, upgrade to `torch>2.4` and enable the [FX graph cache](https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html) with the flag `TORCHINDUCTOR_FX_GRAPH_CACHE=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87be2f-ad9d-41b6-bb0f-fe53eb1cebbd",
   "metadata": {},
   "source": [
    "## Benchmarking with Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962207b2-c7f1-4132-b16c-ed7e320d81d0",
   "metadata": {},
   "source": [
    "We're now ready to re-run our benchmark using the compiled implementation. The only change we'll make is using the [scaled dot product attention (SDPA) context manager](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html) to switch the attention implementation from flash attention to the native PyTorch C++ implementation, which typically leads to better performance under compile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fcf5007-80df-41ec-baa7-e8f0c3a894d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 73/73 [00:10<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.95268702507019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "inference_time = 0.0\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    # 1. Pre-process the audio inputs\n",
    "    input_features = processor(sample[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "    input_features = input_features.to(device, dtype=torch_dtype)\n",
    "    \n",
    "    # 2. Auto-regressively generate text tokens\n",
    "    start = time.time()\n",
    "    with sdpa_kernel(SDPBackend.MATH):\n",
    "        pred_ids = model.generate(input_features)\n",
    "    inference_time += time.time() - start\n",
    "\n",
    "    # 3. Post-process tokens to text\n",
    "    pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "print(inference_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a0a35-1880-49fb-bbcd-6b03d00524ed",
   "metadata": {},
   "source": [
    "The inference speed is reduced to just **9.0-seconds**, representing a speed-up of 3.9x with no degradation to model accuracy ‚ö°Ô∏è\n",
    "\n",
    "Recall that this optimization technique is model-agnostic: it can be applied to any Whisper model in the Transformers library. The largest speed-ups will typically be seen for smaller models. However, even the largest Whisper model ([large-v3](https://huggingface.co/openai/whisper-large-v3)) obtains >3x speed-up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c82a4-99d3-4ee5-a121-fe1ca058c41a",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba867e31-7e8a-4d6a-b22a-c358af7c10ea",
   "metadata": {},
   "source": [
    "In this Colab, we've broken down the steps for Whisper inference using torch compile, demonstrating a 3.5x speed-up from two lines of additional code. For an end-to-end code example, refer to the [Whisper model card](https://huggingface.co/openai/whisper-large-v3#torch-compile)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
