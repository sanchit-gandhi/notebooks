{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0634464a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/gemma-transformers-streamlined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
   "metadata": {
    "id": "bfef752c-1a66-4240-bacf-c6432ab77354"
   },
   "source": [
    "#Â Gemma in ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LIxx0Gt0AY72",
   "metadata": {
    "id": "LIxx0Gt0AY72"
   },
   "source": [
    "Set-up Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vN61p-AEBSP3",
   "metadata": {
    "id": "vN61p-AEBSP3"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536f4eb-98ae-4a0b-a738-77386833af6f",
   "metadata": {},
   "source": [
    "Define quantization config for 4-bit inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57555a2d-fbd3-46be-b3b8-ddb0e223ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e292d7cd-5cfd-44d3-8ea6-8522f29f3912",
   "metadata": {},
   "source": [
    "Load models from pre-trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15",
   "metadata": {
    "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1840.01it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\", low_cpu_mem_usage=True, quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e567-002f-4628-be30-c188a8abfbe9",
   "metadata": {},
   "source": [
    "Step 1: encode the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "235ed7dc-bcff-4c3e-a185-ccdc58a8bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Recipe for pasta:\", return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00b939-ae17-4635-88c9-c17a63bbcf38",
   "metadata": {},
   "source": [
    "Step 2: auto-regressively generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5e25d2fb-ed31-4615-aa90-3002b4c177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "pred_ids = model.generate(input_ids, do_sample=True, temperature=0.6, max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f725ed-6202-4bd3-a97b-759744d48363",
   "metadata": {},
   "source": [
    "Step 3: decode the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "65884039-2319-4df9-9a31-ce73cd43b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe for pasta:\n",
      "\n",
      "1. Boil 1.50 ltr of water\n",
      "2. Add 100 gms of butter and let it melt\n",
      "3. Add 100 gms of all purpose flour and keep stirring\n",
      "4. Once the flour is well blended, add 1/2 ltr of the boiling water and keep stirring\n",
      "5. Keep adding the boiling water a.t.a.t you reach a smooth and stretchable dough\n",
      "6. Keep kneading the dough (keep kneading for atleast 15 mts)\n",
      "7. Once the dough is smooth and stretchable, keep rolling it\n",
      "8. Once the\n"
     ]
    }
   ],
   "source": [
    "pred_text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "print(pred_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d322d-35df-474a-a4bf-465082b2f483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
