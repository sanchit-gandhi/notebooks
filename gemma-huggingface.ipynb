{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/gemma-huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "786c9df3-20bc-42d9-bb8c-2af8d9887ddd",
      "metadata": {
        "id": "786c9df3-20bc-42d9-bb8c-2af8d9887ddd"
      },
      "source": [
        "# Gemma in the Hugging Face Ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4f2dab-1824-4748-ac21-d44a347f37bb",
      "metadata": {
        "id": "ec4f2dab-1824-4748-ac21-d44a347f37bb"
      },
      "source": [
        "The Hugging Face ecosystem is designed to maximise the potential of open-source developers. In this notebook, we'll see how the ecosystem can be leveraged for the \"full-stack\" Gemma pipeline, starting with inference, fine-tuning the model on a specific dataset, and finishing with deploying the model in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8f6e2a-b444-4c8d-8162-59818cd18c5e",
      "metadata": {
        "id": "db8f6e2a-b444-4c8d-8162-59818cd18c5e"
      },
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/sanchit-gandhi/notebooks/blob/main/gemma_pipeline.jpg?raw=true\" width=\"800\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LIxx0Gt0AY72",
      "metadata": {
        "id": "LIxx0Gt0AY72"
      },
      "source": [
        "## Set-up Python environment\n",
        "\n",
        "First, we need to register our Hugging Face Hub token with our Google Colab runtime. Since the Gemma model is gated, our token will be checked when the model is downloaded to ensure we have accepted the terms-of-use. To register your token, click the key symbol ðŸ”‘ in the left-hand pane of the screen. Name the secret `HF_TOKEN`, and copy a token from your Hugging Face [Hub account](https://huggingface.co/settings/tokens). Your token should now be registered, allowing you to access the Gemma weights to this Colab session.\n",
        "\n",
        "For reasonable training and inference speed with Gemma, we'll want to run the model on a GPU. The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all you need to do is hit `\"Connect T4\"` in the top right-hand corner of the screen.\n",
        "\n",
        "Once we've done that, we can go ahead and install the necessary Python packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vN61p-AEBSP3",
      "metadata": {
        "id": "vN61p-AEBSP3"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet transformers huggingface_hub datasets accelerate trl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
      "metadata": {
        "id": "bfef752c-1a66-4240-bacf-c6432ab77354"
      },
      "source": [
        "##Â Inference with Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b41ff769-64fe-45e4-a0e9-0ddd8a845428",
      "metadata": {
        "id": "b41ff769-64fe-45e4-a0e9-0ddd8a845428"
      },
      "source": [
        "[Transformers](https://huggingface.co/docs/transformers/index) is a state-of-the-art toolkit for open-source machine learning models. It contains all the functionality to download pre-trained weights, run inputs through them with inference, and integrations with other libraries for further fine-tuning.\n",
        "\n",
        "There are four pre-trained Gemma checkpoints from which we can choose from, summarised in the table below. All four checkpoints are uploaded to the Hugging Face Hub with integrations in the Transformers library:\n",
        "\n",
        "| Model ID    | Size / B params | Type        |\n",
        "|-------------|-----------------|-------------|\n",
        "| [gemma-2b](https://huggingface.co/google/gemma-2b)    | 2.5             | Base        |\n",
        "| [gemma-2b-it](https://huggingface.co/google/gemma-2b-it) | 2.5             | Instruction |\n",
        "| [gemma-7b](https://huggingface.co/google/gemma-7b)    | 8.5             | Base        |\n",
        "| [gemma-7b-it](https://huggingface.co/google/gemma-7b-it) | 8.5             | Instruction |\n",
        "\n",
        "\n",
        "For this example, we'll use [gemma-2b](https://huggingface.co/google/gemma-2b), the 2B parameter model that has been trained for the task of text generation. An 2B parameter LLM in full precision (float32) requires 10GB of memory just to load the weights, which is already near the limit of the 16GB T4 GPU assigned to Google Colab free tier. To circumvent this, we'll load the weights in [4-bit precision](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which reduces the memory of the weights roughly by a factor of 8.\n",
        "\n",
        "To do this, we'll define a bitsandbytes [quantization config](https://huggingface.co/docs/transformers/quantization#4-bit), which quantifies the precision in which the model should be quantized and what data-type (dtype) to run it in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a538d2aa-2b9a-4049-b5f3-389196887cad",
      "metadata": {
        "id": "a538d2aa-2b9a-4049-b5f3-389196887cad"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c2f3ff1-8cc6-474c-95b0-2312838f78ca",
      "metadata": {
        "id": "9c2f3ff1-8cc6-474c-95b0-2312838f78ca"
      },
      "source": [
        "We can now load the pre-trained model weights from the Hugging Face Hub, passing the quantization config to the loading method to prepare the weights in 4-bit precision:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b296f72-eda4-432f-a09f-e938b77a443a",
      "metadata": {
        "id": "0b296f72-eda4-432f-a09f-e938b77a443a"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "checkpoint_id = \"google/gemma-2b\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint_id, low_cpu_mem_usage=True, quantization_config=quantization_config,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_id, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d78768fa-bd04-4189-9ed2-96bca8254dce",
      "metadata": {
        "id": "d78768fa-bd04-4189-9ed2-96bca8254dce"
      },
      "source": [
        "The advantage of using the [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) and [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) API over model-specific classes is that we can easily swap the checkpoint id for any other checkpoint\n",
        "on the Hugging Face Hub and re-use our code without any changes. The auto-classes will take care of loading the correct model and tokenizer classes for us. That means if a new LLM is released, we can quickly update our code to leverage the new model. Simply swap the checkpoint id `google/gemma-2b` for the model id on the Hugging Face Hub."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41b4e567-002f-4628-be30-c188a8abfbe9",
      "metadata": {
        "id": "41b4e567-002f-4628-be30-c188a8abfbe9"
      },
      "source": [
        "Great! We've loaded our model into memory, so now we're ready to define our inputs for inference. In this example, we'll pass a simple prompt to the Gemma model, and pre-process it to token id representation using our tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "235ed7dc-bcff-4c3e-a185-ccdc58a8bedd",
      "metadata": {
        "id": "235ed7dc-bcff-4c3e-a185-ccdc58a8bedd"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer(\"Recipe for pasta:\", return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a00b939-ae17-4635-88c9-c17a63bbcf38",
      "metadata": {
        "id": "3a00b939-ae17-4635-88c9-c17a63bbcf38"
      },
      "source": [
        "Now that we've pre-processed our inputs, we can auto-regressively generate our response using the model's [`generate`](https://huggingface.co/blog/how-to-generate) method. Here, we'll set generation strategy to sampling, and specify the number of new tokens to generate. A full list of generation parameters can be found [here](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e25d2fb-ed31-4615-aa90-3002b4c177af",
      "metadata": {
        "id": "5e25d2fb-ed31-4615-aa90-3002b4c177af"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "\n",
        "set_seed(0)\n",
        "pred_ids = model.generate(input_ids, do_sample=True, temperature=0.6, max_new_tokens=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38f725ed-6202-4bd3-a97b-759744d48363",
      "metadata": {
        "id": "38f725ed-6202-4bd3-a97b-759744d48363"
      },
      "source": [
        "Finally, we decode the predicted ids back to text characters, again using the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65884039-2319-4df9-9a31-ce73cd43b812",
      "metadata": {
        "id": "65884039-2319-4df9-9a31-ce73cd43b812"
      },
      "outputs": [],
      "source": [
        "pred_text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "print(pred_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "598743a0-af5b-4b00-99d7-7b581e3b4023",
      "metadata": {
        "id": "598743a0-af5b-4b00-99d7-7b581e3b4023"
      },
      "source": [
        "## Datasets with Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "531ba0c6-2e3e-432d-b45a-eda69ea15192",
      "metadata": {
        "id": "531ba0c6-2e3e-432d-b45a-eda69ea15192"
      },
      "source": [
        "[Datasets](https://huggingface.co/docs/datasets/index) is a library for easily accessing and sharing machine learning datasets across all tasks and domains. It can be used to load and pre-process datasets with a single line of code, has powerful functinolity to prepare a dataset ready for training with a transformer-based model. Datasets features a deep integration with the Hugging Face Hub, allowing you to easily load and share a dataset with the wider machine learning community.\n",
        "\n",
        "Let's load a subset of the OpenAssistant dataset from the Hugging Face Hub, [OpenAssistant Guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). This dataset is extremely lightweight (22MB) and so can be downloaded locally very quickly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "731d322d-35df-474a-a4bf-465082b2f483",
      "metadata": {
        "id": "731d322d-35df-474a-a4bf-465082b2f483"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2141747-b551-42fd-aa6c-dc04e2b72e88",
      "metadata": {
        "id": "e2141747-b551-42fd-aa6c-dc04e2b72e88"
      },
      "source": [
        "**Tip:** you can swap this dataset for any one of the [available text datasets](https://huggingface.co/datasets?sort=trending) on the Hugging Face Hub\n",
        "\n",
        "We can pull up the first sample to have a look at the format of our data. We see that the human/assistant turns are labelled by `### Human:` and `### Assistant:` respectively:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac7c20f-4250-4457-9f8e-e77f13b131df",
      "metadata": {
        "id": "1ac7c20f-4250-4457-9f8e-e77f13b131df"
      },
      "outputs": [],
      "source": [
        "sample = dataset[\"train\"][0]\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08b1ea52-eea0-4cc2-a442-00ffe347638e",
      "metadata": {
        "id": "08b1ea52-eea0-4cc2-a442-00ffe347638e"
      },
      "source": [
        "## Training with TRL and PEFT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ec1b2f-00ff-496f-a83f-d83a31fae0fd",
      "metadata": {
        "id": "f4ec1b2f-00ff-496f-a83f-d83a31fae0fd"
      },
      "source": [
        "### TRL\n",
        "\n",
        "[trl](https://huggingface.co/docs/trl/en/index) is an open-source library for training Transformer language models with Reinforcement Learning, from Supervised Fine-Tuning (SFT), to Reward Modelling (RM), and Proximal Policy Optimisation (PPO). The library is built on top of the Transformers Trainer, meaning it is inherently compatible with Transformers models and datasets in Datasets.\n",
        "\n",
        "### PEFT\n",
        "\n",
        "Training LLMs requires a significant amount of GPU compute to hold all the weights, gradients and optimiser states. Specifically, for full fine-tuning, we require:\n",
        "* 2 bytes for the weight\n",
        "* 2 bytes for the gradient\n",
        "* 4 + 8 bytes for the Adam optimizer states\n",
        "\n",
        "With a total of 16 bytes per trainable parameter, this gives a total of 40GB for training the 2B Gemma model (excluding the intermediate hidden states). This means we are at risk of an *out-of-memory (OOM)* error, where the memory usage exceeds that of the GPU.\n",
        "\n",
        "To circumvent this, we'll apply [Parameter Efficient Fine-Funing (PEFT)](https://huggingface.co/docs/peft/index) to only fine-tune a small number of (extra) model parameters and keep the base parameters frozen. This significantly decreases computational and storage costs, while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware, such as Google Colab GPUs.\n",
        "\n",
        "In this example, we'll perform the first step in converting a pre-trained language model to an assitant: [Supervised Fine-Tuning (SFT)](https://huggingface.co/docs/trl/en/sft_trainer). However, the steps covered here can be extended to [Reward Modelling (RM)](https://huggingface.co/docs/trl/en/reward_trainer), [Proximal Policy Optimisation (PPO)](https://huggingface.co/docs/trl/en/ppo_trainer) and [Direct Policy Optimisation (DPO)](https://huggingface.co/docs/trl/en/dpo_trainer). Refer to the linked documentation for examples on each task. For more details on PEFT, refer to this excellent [blog post](https://pytorch.org/blog/finetune-llms/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40f98f6e-58d1-4f86-b788-110895a2cefe",
      "metadata": {
        "id": "40f98f6e-58d1-4f86-b788-110895a2cefe"
      },
      "source": [
        "Let's first define our [training arguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments), such as batch size, number of epochs and learning rate. We'll also specify the output directory for our model, `gemma-2b-fine-tuned`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6338b897-6018-4195-bb81-6c1dd0b202b5",
      "metadata": {
        "id": "6338b897-6018-4195-bb81-6c1dd0b202b5"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    gradient_checkpointing=True,\n",
        "    output_dir=\"./gemma-2b-fine-tuned\",\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ec9783-0ac2-44ec-b0ba-015f5ab1e573",
      "metadata": {
        "id": "b4ec9783-0ac2-44ec-b0ba-015f5ab1e573"
      },
      "source": [
        "Note that if you do not want to push your fine-tuned model to the Hugging Face Hub, set `push_to_hub=False`.\n",
        "\n",
        "We can now define our [PEFT config](https://huggingface.co/docs/peft/en/quicktour#train), which specifies the way in which the extra parameters should be defined and trained during fine-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b991838-590d-4fa1-af40-c7d0ce5c4413",
      "metadata": {
        "id": "4b991838-590d-4fa1-af40-c7d0ce5c4413"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, TaskType\n",
        "\n",
        "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72296e0-668a-4700-b890-63b6571ddb42",
      "metadata": {
        "id": "a72296e0-668a-4700-b890-63b6571ddb42"
      },
      "source": [
        "To instantiate a data-collator for human-assistant style conversation data, pass a response template, an instruction template and the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "930a659e-5188-4a1c-b248-09dfc92e27f4",
      "metadata": {
        "id": "930a659e-5188-4a1c-b248-09dfc92e27f4"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "instruction_template = \"### Human:\"\n",
        "response_template = \"### Assistant:\"\n",
        "\n",
        "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0b81fc-2e92-4bdb-a8af-e5a6e14cb399",
      "metadata": {
        "id": "1f0b81fc-2e92-4bdb-a8af-e5a6e14cb399"
      },
      "source": [
        "We can pass our model, tokenizer and dataset to the [SFT Trainer](https://huggingface.co/docs/trl/sft_trainer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "332abc97-4543-42c1-ba30-66c6dfa98aea",
      "metadata": {
        "id": "332abc97-4543-42c1-ba30-66c6dfa98aea"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    dataset_text_field=\"text\",\n",
        "    data_collator=collator,\n",
        "    max_seq_length=2048,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "897a93b0-030d-4d47-ba8c-540ec323ab43",
      "metadata": {
        "id": "897a93b0-030d-4d47-ba8c-540ec323ab43"
      },
      "source": [
        "And then launch training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6307f0f5-afa2-4534-99c1-9ca2643de15a",
      "metadata": {
        "id": "6307f0f5-afa2-4534-99c1-9ca2643de15a"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44cf823c-3e85-45d2-adce-38b109c6b16d",
      "metadata": {
        "id": "44cf823c-3e85-45d2-adce-38b109c6b16d"
      },
      "source": [
        "Training will take approximately 3-6 hours depending on your GPU or the one allocated to the Google Colab. Depending on your GPU, it may be possible to increase the `per_device_train_batch_size` to increase throughput. In this case, you can increase the `per_device_train_batch_size` incrementally by factors of 2 until you reach the maximum possible batch size. Alternatively, if you are limited to a `per_device_train_batch_size` of 1, you can employ [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps) to compensate, by also increasing them by factors of 2 to bump your effective batch size.\n",
        "\n",
        "Once training has finished, we can push the final model to the Hugging Face Hub, such that it is available under `your-username/gemma-2b-fine-tuned`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "418f1d9b-d6aa-4678-8697-6a182da788fd",
      "metadata": {
        "id": "418f1d9b-d6aa-4678-8697-6a182da788fd"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(\n",
        "    finetuned_from=checkpoint_id,\n",
        "    dataset_tags=\"timdettmers/openassistant-guanaco\",\n",
        "    tasks=\"text-generation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b565c1dc-b724-4d92-913d-f0aee152412a",
      "metadata": {
        "id": "b565c1dc-b724-4d92-913d-f0aee152412a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}