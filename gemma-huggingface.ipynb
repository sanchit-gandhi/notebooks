{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0634464a",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/gemma-transformers-streamlined.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c9df3-20bc-42d9-bb8c-2af8d9887ddd",
   "metadata": {},
   "source": [
    "# Gemma in the Hugging Face Ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4f2dab-1824-4748-ac21-d44a347f37bb",
   "metadata": {},
   "source": [
    "The Hugging Face ecosystem is designed to maximise the potential of open-source developers. In this notebook, we'll see how the ecosystem can be leveraged for the \"full-stack\" Gemma pipeline, starting with inference, fine-tuning the model on a specific dataset, and finishing with deploying the model in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8f6e2a-b444-4c8d-8162-59818cd18c5e",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/sanchit-gandhi/notebooks/blob/main/gemma_pipeline.jpg?raw=true\" width=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LIxx0Gt0AY72",
   "metadata": {
    "id": "LIxx0Gt0AY72"
   },
   "source": [
    "## Set-up Python environment\n",
    "\n",
    "First, we need to register our Hugging Face Hub token with our Google Colab runtime. Since the Gemma model is gated, our token will be checked when the model is downloaded to ensure we have accepted the terms-of-use. To register your token, click the key symbol ðŸ”‘ in the left-hand pane of the screen. Name the secret `HF_TOKEN`, and copy a token from your Hugging Face [Hub account](https://huggingface.co/settings/tokens). Your token should now be registered, allowing you to access the Gemma weights to this Colab session.\n",
    "\n",
    "For reasonable training and inference speed with Gemma, we'll want to run the model on a GPU. The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all you need to do is hit `\"Connect T4\"` in the top right-hand corner of the screen.\n",
    "\n",
    "Once we've done that, we can go ahead and install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vN61p-AEBSP3",
   "metadata": {
    "id": "vN61p-AEBSP3"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet transformers datasets accelerate trl peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
   "metadata": {
    "id": "bfef752c-1a66-4240-bacf-c6432ab77354"
   },
   "source": [
    "##Â Inference with Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ff769-64fe-45e4-a0e9-0ddd8a845428",
   "metadata": {},
   "source": [
    "[Transformers](https://huggingface.co/docs/transformers/index) is a state-of-the-art toolkit for open-source machine learning models. It contains all the functionality to download pre-trained weights, run inputs through them with inference, and integrations with other libraries for further fine-tuning.\n",
    "\n",
    "There are four pre-trained Gemma checkpoints from which we can choose from, summarised in the table below. All four checkpoints are uploaded to the Hugging Face Hub with integrations in the Transformers library:\n",
    "\n",
    "| Model ID    | Size / B params | Type        |\n",
    "|-------------|-----------------|-------------|\n",
    "| [gemma-2b](https://huggingface.co/google/gemma-2b)    | 2.5             | Base        |\n",
    "| [gemma-2b-it](https://huggingface.co/google/gemma-2b-it) | 2.5             | Instruction |\n",
    "| [gemma-7b](https://huggingface.co/google/gemma-7b)    | 8.5             | Base        |\n",
    "| [gemma-7b-it](https://huggingface.co/google/gemma-7b-it) | 8.5             | Instruction |\n",
    "\n",
    "\n",
    "For this example, we'll use [gemma-2b](https://huggingface.co/google/gemma-2b), the 2B parameter model that has been trained for the task of text generation. An 2B parameter LLM in full precision (float32) requires 10GB of memory just to load the weights, and a whopping 40GB for fine-tuning! The GPU typically assigned to a Google Colab free tier only has a capacity of 16GB. This means we are at risk of an *out-of-memory (OOM)* error, where the memory usage exceeds that of the GPU. To circumvent this, we'll load the weights in [4-bit precision](https://huggingface.co/blog/4bit-transformers-bitsandbytes), which reduces the memory of the weights roughly by a factor of 8. \n",
    "\n",
    "To do this, we'll define a bitsandbytes [quantization config](https://huggingface.co/docs/transformers/quantization#4-bit), which quantifies the precision in which the model should be quantized and what data-type (dtype) to run it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a538d2aa-2b9a-4049-b5f3-389196887cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/hf/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sanchit/transformers/src/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f3ff1-8cc6-474c-95b0-2312838f78ca",
   "metadata": {},
   "source": [
    "We can now load the pre-trained model weights from the Hugging Face Hub, passing the quantization config to the loading method to prepare the weights in 4-bit precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b296f72-eda4-432f-a09f-e938b77a443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 3611.11it/s]\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\", low_cpu_mem_usage=True, quantization_config=quantization_config,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78768fa-bd04-4189-9ed2-96bca8254dce",
   "metadata": {},
   "source": [
    "The advantage of using the [`AutoModelForCausalLM`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM) and [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) API over model-specific classes is that we can easily swap the checkpoint id for any other checkpoint\n",
    "on the Hugging Face Hub and re-use our code without any changes. The auto-classes will take care of loading the correct model and tokenizer classes for us. That means if a new LLM is released, we can quickly update our code to leverage the new model. Simply swap the checkpoint id `google/gemma-2b` for the model id on the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4e567-002f-4628-be30-c188a8abfbe9",
   "metadata": {},
   "source": [
    "Great! We've loaded our model into memory, so now we're ready to define our inputs for inference. In this example, we'll pass a simple prompt to the Gemma model, and pre-process it to token id representation using our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235ed7dc-bcff-4c3e-a185-ccdc58a8bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Recipe for pasta:\", return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00b939-ae17-4635-88c9-c17a63bbcf38",
   "metadata": {},
   "source": [
    "Now that we've pre-processed our inputs, we can auto-regressively generate our response using the model's [`generate`](https://huggingface.co/blog/how-to-generate) method. Here, we'll set generation strategy to sampling, and specify the number of new tokens to generate. A full list of generation parameters can be found [here](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e25d2fb-ed31-4615-aa90-3002b4c177af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(0)\n",
    "pred_ids = model.generate(input_ids, do_sample=True, temperature=0.6, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f725ed-6202-4bd3-a97b-759744d48363",
   "metadata": {},
   "source": [
    "Finally, we decode the predicted ids back to text characters, again using the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65884039-2319-4df9-9a31-ce73cd43b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe for pasta:\n",
      "\n",
      "* 4 1/2 cups of all-purpose flour\n",
      "* 2 tsp of salt\n",
      "* 2 tsp of sugar\n",
      "* 12 oz. of warm water\n",
      "* 2 tsp of active dry yeast\n",
      "* 3 tbsp of vegetable oil\n",
      "* 3/4 tsp of granulated garlic\n",
      "* 1 tsp of granulated onion\n",
      "* 1/2 tsp of dried marjoram\n",
      "* 1/4 tsp of ground black pepper\n",
      "* 1/4 tsp of cayenne pepper\n",
      "\n",
      "In a large bowl, sift together the flour, salt, sugar, and garlic, onion, and marjoram. Make a well in the dry ingredients and pour in the yeast. Add the oil and use your hands to mix.\n",
      "\n",
      "Add the water and knead the dough together into a smooth, elastic, and extensible dough.\n",
      "\n",
      "Cover the dough and let it rise for about 20 minutes.\n",
      "\n",
      "Divide the dough into two or three loaves and shape them into your desired shape.\n",
      "\n",
      "Cover the loaves and let them rise for about 30 minutes.\n",
      "\n",
      "Preheat the oven to 375Â°F.\n",
      "\n",
      "Brush a little oil or melted butter on the baking sheet and place the loaves on the sheet.\n",
      "\n",
      "Stick a\n"
     ]
    }
   ],
   "source": [
    "pred_text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "print(pred_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598743a0-af5b-4b00-99d7-7b581e3b4023",
   "metadata": {},
   "source": [
    "## Datasets with Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ba0c6-2e3e-432d-b45a-eda69ea15192",
   "metadata": {},
   "source": [
    "[Datasets](https://huggingface.co/docs/datasets/index) is a library for easily accessing and sharing machine learning datasets across all tasks and domains. It can be used to load and pre-process datasets with a single line of code, has powerful functinolity to prepare a dataset ready for training with a transformer-based model. Datasets features a deep integration with the Hugging Face Hub, allowing you to easily load and share a dataset with the wider machine learning community.\n",
    "\n",
    "Let's load a subset of the OpenAssistant dataset from the Hugging Face Hub, [OpenAssistant Guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco). This dataset is extremely lightweight (22MB) and so can be downloaded locally very quickly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731d322d-35df-474a-a4bf-465082b2f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/hf/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2141747-b551-42fd-aa6c-dc04e2b72e88",
   "metadata": {},
   "source": [
    "**Tip:** you can swap this dataset for any one of the [available text datasets](https://huggingface.co/datasets?sort=trending) on the Hugging Face Hub\n",
    "\n",
    "We can pull up the first sample to have a look at the format of our data. We see that the human/assistant turns are labelled by `### Human:` and `### Assistant:` respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ac7c20f-4250-4457-9f8e-e77f13b131df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = dataset[\"train\"][0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1ea52-eea0-4cc2-a442-00ffe347638e",
   "metadata": {},
   "source": [
    "## Training with TRL and PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec1b2f-00ff-496f-a83f-d83a31fae0fd",
   "metadata": {},
   "source": [
    "[trl](https://huggingface.co/docs/trl/en/index) is an open-source library for training Transformer language models with Reinforcement Learning, from Supervised Fine-Tuning (SFT), to Reward Modelling (RM), and Proximal Policy Optimisation (PPO). The library is built on top of the Transformers Trainer, meaning it is inherently compatible with Transformers models and datasets in Datasets.\n",
    "\n",
    "As mentioned above, full fine-tuning of the Gemma model would require 40GB of GPU memory. To circumvent this, we'll apply [Parameter Efficient Fine-Funing (PEFT)](https://huggingface.co/docs/peft/index) to only fine-tune a small number of (extra) model parameters. This significantly decreases computational and storage costs, while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware, such as Google Colab GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f98f6e-58d1-4f86-b788-110895a2cefe",
   "metadata": {},
   "source": [
    "Let's first define our [training arguments](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/trainer#transformers.TrainingArguments), such as batch size, number of epochs and learning rate. We'll also specify the output directory for our model, `gemma-2b-fine-tuned`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6338b897-6018-4195-bb81-6c1dd0b202b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    gradient_checkpointing=True,\n",
    "    output_dir=\"gemma-2b-fine-tuned\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ec9783-0ac2-44ec-b0ba-015f5ab1e573",
   "metadata": {},
   "source": [
    "Note that if you do not want to push your fine-tuned model to the Hugging Face Hub, set `push_to_hub=False`.\n",
    "\n",
    "We can now define our [PEFT config](https://huggingface.co/docs/peft/en/quicktour#train), which specifies the way in which the extra parameters should be defined and trained during fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b991838-590d-4fa1-af40-c7d0ce5c4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159020ca-577d-4cac-bf54-39778b9ff5a5",
   "metadata": {},
   "source": [
    "And also prepare our model for 4-bit training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e8a3be9-1d89-416c-b673-e2631de3f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72296e0-668a-4700-b890-63b6571ddb42",
   "metadata": {},
   "source": [
    "To instantiate a data-collator for human-assistant style conversation data, pass a response template, an instruction template and the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "930a659e-5188-4a1c-b248-09dfc92e27f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/hf/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/sanchit/hf/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b81fc-2e92-4bdb-a8af-e5a6e14cb399",
   "metadata": {},
   "source": [
    "We can pass our model, tokenizer and dataset to the [SFT Trainer](https://huggingface.co/docs/trl/sft_trainer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "332abc97-4543-42c1-ba30-66c6dfa98aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9846/9846 [00:01<00:00, 6157.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 518/518 [00:00<00:00, 5736.41 examples/s]\n",
      "/home/sanchit/hf/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    "    max_seq_length=min(model.config.max_position_embeddings, tokenizer.model_max_length),\n",
    "    peft_config=peft_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a93b0-030d-4d47-ba8c-540ec323ab43",
   "metadata": {},
   "source": [
    "And then launch training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6307f0f5-afa2-4534-99c1-9ca2643de15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='3690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  21/3690 00:35 < 1:54:50, 0.53 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.093600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf823c-3e85-45d2-adce-38b109c6b16d",
   "metadata": {},
   "source": [
    "Training will take approximately 2-3 hours depending on your GPU or the one allocated to the Google Colab. Depending on your GPU, it is possible that you will encounter a CUDA `\"out-of-memory\"` error when you start training. In this case, you can reduce the `per_device_train_batch_size` incrementally by factors of 2 and employ [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps) to compensate.\n",
    "\n",
    "Once training has finished, we can push the final model to the Hugging Face Hub, such that it is available under `your-username/gemma-2b-fine-tuned`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f1d9b-d6aa-4678-8697-6a182da788fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\n",
    "    finetuned_from=checkpoint_id,\n",
    "    dataset_tags=\"timdettmers/openassistant-guanaco\",\n",
    "    tasks=\"text-generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b565c1dc-b724-4d92-913d-f0aee152412a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
