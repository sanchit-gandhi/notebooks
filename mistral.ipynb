{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
   "metadata": {},
   "source": [
    "# Build a chat bot in 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57289a71-58f3-4801-8595-27585c300392",
   "metadata": {},
   "source": [
    "Since the release of Open AI's [ChatGPT](https://openai.com/blog/chatgpt) in November 2022, machine learning has been thrust into the public eye. \n",
    "The power and versatility of Large Language Models (LLMs) have been brought to the forefront, showcasing their ability to understand and generate human-like text with unprecedented accuracy. This advancement opens up exciting possibilities for developing sophisticated chatbots, assistants and \n",
    "\n",
    "In this tutorial, we will leverage an open-source LLM to build a chatbot in under 10 minutes. This exercise not only demonstrates the practical application of LLMS in\n",
    "creating interactive and intelligent conversational agents, but also emphasises their potential in enhancing customer engagement, automating inquiries, and providing real-time assistance in finance-related contexts. You'll gain first-hand experience in harnessing the power of LLMs, marking a significant step towards integrating cutting-edge AI to financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b178891-40ce-4055-9d64-fbf860fc4278",
   "metadata": {},
   "source": [
    "## Baseline Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9810b5d-0507-4d9e-bbca-d22a9e8b4942",
   "metadata": {},
   "source": [
    "In this example, we'll use Transformers' lower-level model + tokenizer API. Let's go ahead an import the two Python classes we'll need to run the Mistral model. The first of these classes, `AutoModelForCausalLM`, is our \n",
    "model class. This is the Python class that holds the model weights and graph definition. The second class, `AutoTokenizer`, is the tokenizer class,\n",
    "which converts the input string to a token id (discrete number) representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433d9474-6b18-41e7-9d53-f9741bc5cbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62893926-b201-445f-b548-10b5ed643ab1",
   "metadata": {},
   "source": [
    "We can now load the pre-trained model weights from the Hugging Face Hub. For this example, we'll use [Mistral's 7B LLM](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) that has been instructioned tuned for the task of a chatbot. Now, a 7B parameter LLM in full precision (float32) requires 16GB of memory just to load the weights. The GPU typically assigned to a Google Colab free tier only has a capacity of 16GB. This means we are at risk of an *out-of-memory (OOM)* error, where the memory of the model exceeds that of the GPU. To circumvent this, we'll load the weights in [4-bit precision]((https://huggingface.co/blog/4bit-transformers-bitsandbytes)), which should reduce the memory of the weights roughly by a factor of 8. This is extremely simple in Transformers: just pass the flag `load_in_4bit` when we load the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:33<00:00, 17.00s/it]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_id, low_cpu_mem_usage=True, load_in_4bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecec8ce-0997-4b66-a98d-0b72da4c6bac",
   "metadata": {},
   "source": [
    "The advantage of using the `AutoModel` and `AutoTokenizer` API over model-specific classes is that we can easily swap the checkpoint id for any other checkpoint \n",
    "on the Hugging Face Hub and re-use our code without any changes. The auto-classes will take care of loading the correct model and tokenizer classes for us! That means if a better LLM is released (highly probable!), we can quickly update our code to leverage the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc086ea-aa85-444d-a3ee-11eb8f719f71",
   "metadata": {},
   "source": [
    "Great! We've loaded our model into memory, so now we're ready to define our pre-processing strategy. This looks a bit different to what we had previously when we did summarisation. Instead of just tokenising the input text, we need to tokenise a series of user/assistant exchanges. The precise formatting for tokenisation is quite \n",
    "fiddly. Luckily for us, the tokenizer provides a convinient method [`apply_chat_template`](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) to format the tokens correctly for us. We just need to define what messages were sent by the user, and what ones were provided by the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58984579-0043-4bae-81af-fc242e8f2306",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me about the current trends in the stock market?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Certainly! Recently, there's been a growing interest in technology stocks, particularly those involved in artificial intelligence and renewable energy. However, it's important to consider market volatility and global economic factors in your investment decisions.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What should I consider when investing in tech stocks?\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a398fc4f-c439-4050-a2e9-da807717318b",
   "metadata": {},
   "source": [
    "Perfect! Now, our model is currently sitting on the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05466436-f050-45c8-9f7e-23e8b9acf74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6628ed-1d58-49bf-8f18-f5365ad34426",
   "metadata": {},
   "source": [
    "Whereas our inputs are on the CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb986f71-a374-4606-88bd-f8aaff754f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodeds.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa895911-1fa0-4a44-9d6e-478eff4154ac",
   "metadata": {},
   "source": [
    "Every PyTorch model expects the inputs to be on the same device as the model. If the model is a race car and we are the passengers, we need to be in the race car before we can get going! Let's move the inputs to the correct device before generating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1eb655f-d621-4a58-b400-dd78b1acf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = encodeds.to(model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e81e4-22d2-49ff-81a7-a1b3ecd4d47a",
   "metadata": {},
   "source": [
    "Now that we've pre-processed our inputs, we can generate our response using the model in much the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc53409-a5e5-4e3d-8129-209be763f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] Can you tell me about the current trends in the stock market? [/INST]Certainly! Recently, there's been a growing interest in technology stocks, particularly those involved in artificial intelligence and renewable energy. However, it's important to consider market volatility and global economic factors in your investment decisions.  [INST] What should I consider when investing in tech stocks? [/INST] When investing in technology stocks, it's important to consider factors such as:\n",
      "\n",
      "1. Market trends: Keep an eye on the overall market trends and how they may impact the industry and specific companies.\n",
      "2. Growth prospects: Look for companies that have strong growth potential and a competitive advantage in their industry.\n",
      "3. Revenue and earnings: Consider the financial performance of the company through its revenue and earnings growth.\n",
      "4. Product and service offerings: Analyze the products and services offered by the company and their potential for market demand and profitability.\n",
      "5. Management team: Evaluate the management team's experience and track record in the industry.\n",
      "6. Industry and competitive landscape: Understand the competitive environment and any potential regulatory or geopolitical risks that may affect the industry.\n",
      "\n",
      "It's also a good idea to diversify your investments in technology stocks, rather than investing heavily in one particular stock or sector.\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True)\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af8fc47-4023-4120-86d7-4c31a12435bc",
   "metadata": {},
   "source": [
    "Looks like a very reasonable response! We can see the change between the user input and the assistant response marked by the `[INST]` and `[/INST]` tokens. This was handled automatically for us by the tokenizer. Note that we've generated with [*sampling*](https://huggingface.co/blog/how-to-generate#sampling), so there's an element of randomness in the generations, meaining each one will be different from the last. Feel free to re-run generation a few times to get a feel for the kind of variety that sampling brings.\n",
    "\n",
    "> **Note:** the responses generated by the model are purely for demonstration purposes. They should not be taken as financial recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85bc5e-faf6-4f35-b22f-212fdd3eda9e",
   "metadata": {},
   "source": [
    "## Streaming Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065190a3-4062-45f3-bf91-918e7411b8dc",
   "metadata": {},
   "source": [
    "While the generated response looks good, we had to wait a significant amount of time for the model to finish generating. We know from our knowledge of Transformer models that they generate in an *auto-regressive* fashion, that is, one token at a time:\n",
    "\n",
    "<figure class=\"image table text-center m-0 w-full\">\n",
    "    <video\n",
    "        style=\"max-width: 90%; margin: auto;\"\n",
    "        autoplay loop muted playsinline\n",
    "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
    "    ></video>\n",
    "</figure>\n",
    "\n",
    "Rather than waiting for the entire text sequence to finish generating, we can print each token as it is generated. While this doesn't change the overall latency of the model (the total generation time is the same as before), the *percieved* latency is lower, since the first generated token is returned to the user as soon as it's ready.\n",
    "\n",
    "Transformers provides a [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer) class to easily print the predictions on-the-fly. We simply have to pass the streamer to our generate function, and the rest is taken care for us. Let's run an example using the text streamer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be7896d-160a-40f5-a5f0-72b382bea2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] Can you tell me about the current trends in the stock market? [/INST]Certainly! Recently, there's been a growing interest in technology stocks, particularly those involved in artificial intelligence and renewable energy. However, it's important to consider market volatility and global economic factors in your investment decisions.</s>  [INST] What should I consider when investing in tech stocks? [/INST] When investing in tech stocks, it is important to consider several factors:\n",
      "\n",
      "1. Market trends: Keep track of the latest market trends, including global economic factors, interest rates, and technological innovations.\n",
      "2. Company performance: Research the financial health, growth potential, and product development of the company you are considering investing in.\n",
      "3. Competition: Study the competitive landscape and assess the company's competitive advantage compared to its rivals.\n",
      "4. Valuation: Determine whether the stock is currently fairly valued based on factors such as price-to-earnings ratio (P/E) and revenue growth.\n",
      "5. Management team: Assess the quality and experience of the management team, who can guide the company through tough times and capitalize on new opportunities.\n",
      "6. Risks: Be aware that the technology industry is prone to rapid changes and unexpected events, so it's important to manage risk effectively.\n",
      "\n",
      "It's also worth noting that past performance and trends are not guaranteed indicators of future results, so it's important to do your own research and seek professional advice before making any investment decisions.</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0aac2a-5458-4f33-a54c-f39378dd184c",
   "metadata": {},
   "source": [
    "Interacting with the LLM this way already feels more personalised, as we watch it generate each token at a time. We can ask the model to perform a more advanced task by modifying the prompt. In the following example, we'll provide the model with a summary of [Apple's Q3 Earnings Report](https://www.apple.com/uk/newsroom/2023/08/apple-reports-third-quarter-results/), and ask it to provide a summary of the potential implications on the US economy. This is an example of *prompt engineering*, where we control the functionality of the model by changing the form of the input prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0bf899-624c-4cd3-b968-d5b12d5e92ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] I have the latest earnings report of Apple. Based on this report's financial performance indicators, including revenue, profits, and growth projections, can you analyze and advise on the potential impact this company's performance might have on the broader US economy? Please consider factors such as the company's market influence, sector trends, and any ripple effects that might be observed in related industries or the economy as a whole. Here is the report: Apple today announced financial results for its fiscal 2023 third quarter ended July 1, 2023. The Company posted quarterly revenue of $81.8 billion, down 1 percent year over year, and quarterly earnings per diluted share of $1.26, up 5 percent year over year. 'We are happy to report that we had an all-time revenue record in Services during the June quarter, driven by over 1 billion paid subscriptions, and we saw continued strength in emerging markets thanks to robust sales of iPhone,' said Tim Cook, Apple’s CEO. 'From education to the environment, we are continuing to advance our values, while championing innovation that enriches the lives of our customers and leaves the world better than we found it.''Our June quarter year-over-year business performance improved from the March quarter, and our installed base of active devices reached an all-time high in every geographic segment,' said Luca Maestri, Apple’s CFO. 'During the quarter, we generated very strong operating cash flow of $26 billion, returned over $24 billion to our shareholders, and continued to invest in our long-term growth plans.Apple’s board of directors has declared a cash dividend of $0.24 per share of the Company’s common stock. The dividend is payable on August 17, 2023 to shareholders of record as of the close of business on August 14, 2023. [/INST] Based on the earnings report of Apple for its third quarter of fiscal 2023, it can be observed that the company had a strong financial performance. Despite a 1% decline in revenue compared to the previous year, the company generated over $26 billion in operating cash flow and returned $24 billion to its shareholders. Additionally, Apple's installed base of active devices reached an all-time high in every geographic segment, indicating that the company is expanding its customer base.\n",
      "\n",
      "Apple's market influence should not be underestimated, as the company is a significant player in the technology industry. Its products and services are widely used and have a significant impact on the economy. For example, the demand for iPhones and other Apple devices can drive sales in related industries such as mobile carriers and accessories manufacturers.\n",
      "\n",
      "Furthermore, Apple's financial performance can have a ripple effect on the broader US economy. As a company that operates globally, it can influence the economic conditions of various regions, including the US. Apple's profitability can lead to increased investments in research and development, which can, in turn, drive innovation and growth in the technology industry. This growth can lead to job creation and increased consumer spending, which, in turn, can stimulate the broader economy.\n",
      "\n",
      "Overall, Apple's financial performance can have both direct and indirect impacts on the US economy. As a significant player in the technology industry, it can influence the economic conditions of various regions, including the US. Its profitability can drive innovation and growth, leading to job creation and increased consumer spending.</s>\n"
     ]
    }
   ],
   "source": [
    "APPLE_EARNINGS = (\n",
    "    \"Apple today announced financial results for its fiscal 2023 third quarter ended July 1, 2023. The Company posted quarterly revenue of $81.8 billion, down 1 percent year over year, and quarterly earnings per diluted share of $1.26, up 5 percent year over year. \"\n",
    "    \"'We are happy to report that we had an all-time revenue record in Services during the June quarter, driven by over 1 billion paid subscriptions, and we saw continued strength in emerging markets thanks to robust sales of iPhone,' said Tim Cook, Apple’s CEO. 'From education to the environment, we are continuing to advance our values, while championing innovation that enriches the lives of our customers and leaves the world better than we found it.'\"\n",
    "    \"'Our June quarter year-over-year business performance improved from the March quarter, and our installed base of active devices reached an all-time high in every geographic segment,' said Luca Maestri, Apple’s CFO. 'During the quarter, we generated very strong operating cash flow of $26 billion, returned over $24 billion to our shareholders, and continued to invest in our long-term growth plans.\"\n",
    "    \"Apple’s board of directors has declared a cash dividend of $0.24 per share of the Company’s common stock. The dividend is payable on August 17, 2023 to shareholders of record as of the close of business on August 14, 2023.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"I have the latest earnings report of Apple. Based on this report's financial performance indicators, including revenue, profits, and growth projections, can you analyze and advise on the potential impact this company's performance might have on the broader US economy? Please consider factors such as the company's market influence, sector trends, and any ripple effects that might be observed in related industries or the economy as a whole. Here is the report: {APPLE_EARNINGS}\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "model_inputs = encodeds.to(model.device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ada74-36c1-49d1-9d64-7452af25488f",
   "metadata": {},
   "source": [
    "## Gradio Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86cf1d-49d6-4e92-b63b-0e492cc8fe4d",
   "metadata": {},
   "source": [
    "While it's interesting for us learners to interact with the model through Python code, it's by no means the easiest way to share our model with others. To finish this section, we'll demonstrate how the Mistral model can be wrapped into a simple chat interface and shared with anyone in the community.\n",
    "\n",
    "To achieve this, we'll use the [Gradio library](https://www.gradio.app). Gradio is an open-source Python library that simplifies the creation of web interfaces for machine learning models. It allows developers and researchers to quickly build and share customisable UI components for model testing and feedback, enhancing the accessibility and usability of AI models.\n",
    "\n",
    "First, we'll define an end-to-end function that takes the user's message and the chat history, and generates a response. Largely speaking, this simply requires concatenating the three stages of prediction:\n",
    "1. Tokenizer (pre-process) the message and chat history\n",
    "2. Define the generation arguments\n",
    "3. Generate the predicted ids and stream the output\n",
    "\n",
    "We'll make a small modification to our streamer: instead of using the base [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer), we'll use the [`TextIteratorStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer), which returns the generated ids in a non-blocking way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b62bebe7-27da-4255-8429-bdcd0033b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "\n",
    "MAX_INPUT_TOKEN_LENGTH = 4096\n",
    "\n",
    "def generate(message, chat_history):\n",
    "    # Step 1: pre-process the inputs\n",
    "    conversation = []\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "\n",
    "    # in-case our inputs exceed the maximum length, we might need to cut them\n",
    "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "        gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Step 2: define generation arguments\n",
    "    generate_kwargs = dict(\n",
    "        {\"input_ids\": input_ids},\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    # Step 3: generate and stream outputs\n",
    "    outputs = \"\"\n",
    "    for text in streamer:\n",
    "        outputs += text\n",
    "        yield outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5112d2-a068-4acc-838d-b07c1052b8c8",
   "metadata": {},
   "source": [
    "With the generation function defined, we can now create a Gradio demo in just 3 lines of additional code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d407a6d9-9eef-4a27-97c4-89dec1d10bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://152b6e4f4167b4c57a.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://152b6e4f4167b4c57a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanchit/hf/lib/python3.10/site-packages/gradio/components/button.py:89: UserWarning: Using the update method is deprecated. Simply return a new object instead, e.g. `return gr.Button(...)` instead of `return gr.Button.update(...)`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "chat_interface = gr.ChatInterface(generate)\n",
    "chat_interface.queue().launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af8af6-f499-400d-a784-9277be0e6c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
