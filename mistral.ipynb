{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfef752c-1a66-4240-bacf-c6432ab77354",
      "metadata": {
        "id": "bfef752c-1a66-4240-bacf-c6432ab77354"
      },
      "source": [
        "#Â Build a chat bot in 10 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57289a71-58f3-4801-8595-27585c300392",
      "metadata": {
        "id": "57289a71-58f3-4801-8595-27585c300392"
      },
      "source": [
        "Since the release of Open AI's [ChatGPT](https://openai.com/blog/chatgpt) in November 2022, machine learning has been thrust into the public eye.\n",
        "The power and versatility of Large Language Models (LLMs) have been brought to the forefront, showcasing their ability to understand and generate human-like text with unprecedented accuracy. This advancement opens up exciting possibilities for developing sophisticated chatbots and human-assistants.\n",
        "\n",
        "In this tutorial, we will leverage the [ðŸ¤— Transformers](https://huggingface.co/docs/transformers/index) library and an open-source LLM to build a powerful chatbot in under 10 minutes. Unlike many closed-source chatbots, like ChatGPT, the chatbot that we build will be **fully open-source**. This means that you have full control over the model, it's weights and the generation parameters. Furthermore, any user responses will be kept locally and not transmitted to a third party. This is helpful for confidential applications, such as those in a financial domain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Set-Up Environment"
      ],
      "metadata": {
        "id": "LIxx0Gt0AY72"
      },
      "id": "LIxx0Gt0AY72"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're using larger models than the previous tutorial, we'll need to run them on a GPU. The runtime is already configured to use the free 16GB T4 GPU provided through Google Colab Free Tier, so all you need to do is hit \"Connect T4\" in the top right-hand corner of the screen.\n",
        "\n",
        "Once we've done that, we can go ahead and install the necessary Python packages:"
      ],
      "metadata": {
        "id": "0NOicgrIBMo_"
      },
      "id": "0NOicgrIBMo_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet transformers accelerate bitsandbytes gradio"
      ],
      "metadata": {
        "id": "vN61p-AEBSP3"
      },
      "id": "vN61p-AEBSP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8b178891-40ce-4055-9d64-fbf860fc4278",
      "metadata": {
        "id": "8b178891-40ce-4055-9d64-fbf860fc4278"
      },
      "source": [
        "##Â Baseline Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9810b5d-0507-4d9e-bbca-d22a9e8b4942",
      "metadata": {
        "id": "f9810b5d-0507-4d9e-bbca-d22a9e8b4942"
      },
      "source": [
        "In this example, we'll use Transformers' lower-level model + tokenizer API. Let's go ahead an import the two Python classes we'll need to run the Mistral model. The first of these classes, `AutoModelForCausalLM`, is our\n",
        "model class. This is the Python class that holds the model weights and graph definition. The second class, `AutoTokenizer`, is the tokenizer class,\n",
        "which converts the input string to a token id (discrete number) representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "433d9474-6b18-41e7-9d53-f9741bc5cbfa",
      "metadata": {
        "id": "433d9474-6b18-41e7-9d53-f9741bc5cbfa"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62893926-b201-445f-b548-10b5ed643ab1",
      "metadata": {
        "id": "62893926-b201-445f-b548-10b5ed643ab1"
      },
      "source": [
        "We can now load the pre-trained model weights from the Hugging Face Hub. For this example, we'll use [Mistral's 7B LLM](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) that has been instructioned tuned for the task of a chatbot.\n",
        "\n",
        "Now, a 7B parameter LLM in full precision (float32) requires 16GB of memory just to load the weights. The GPU typically assigned to a Google Colab free tier only has a capacity of 16GB. This means we are at risk of an *out-of-memory (OOM)* error, where the memory of the model exceeds that of the GPU. To circumvent this, we'll load the weights in [4-bit precision]((https://huggingface.co/blog/4bit-transformers-bitsandbytes)), which should reduce the memory of the weights roughly by a factor of 8. This is extremely simple in Transformers: just pass the flag [`load_in_4bit`](https://huggingface.co/docs/transformers/main_classes/model#transformers.PreTrainedModel.from_pretrained.load_in_4bit) when we load the pre-trained weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15",
      "metadata": {
        "id": "20dd3de1-e117-47ac-ab90-125d4c47dd15"
      },
      "outputs": [],
      "source": [
        "checkpoint_id = \"sanchit-gandhi/Mistral-7B-Instruct-v0.1\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint_id, low_cpu_mem_usage=True, load_in_4bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cecec8ce-0997-4b66-a98d-0b72da4c6bac",
      "metadata": {
        "id": "cecec8ce-0997-4b66-a98d-0b72da4c6bac"
      },
      "source": [
        "The advantage of using the `AutoModel` and `AutoTokenizer` API over model-specific classes is that we can easily swap the checkpoint id for any other checkpoint\n",
        "on the Hugging Face Hub and re-use our code without any changes. The auto-classes will take care of loading the correct model and tokenizer classes for us! That means if a better LLM is released (highly probable!), we can quickly update our code to leverage the new model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffc086ea-aa85-444d-a3ee-11eb8f719f71",
      "metadata": {
        "id": "ffc086ea-aa85-444d-a3ee-11eb8f719f71"
      },
      "source": [
        "Great! We've loaded our model into memory, so now we're ready to define our pre-processing strategy. This looks a bit different to what we had previously when we did summarisation. Instead of just tokenising the input text, we need to tokenise a series of user/assistant exchanges. The precise formatting for tokenisation is quite\n",
        "fiddly. Luckily for us, the tokenizer provides a convinient method [`apply_chat_template`](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) to format the tokens correctly for us. We just need to define what messages were sent by the user, and what ones were provided by the assistant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58984579-0043-4bae-81af-fc242e8f2306",
      "metadata": {
        "id": "58984579-0043-4bae-81af-fc242e8f2306"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me about the current trends in the stock market?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Certainly! Recently, there's been a growing interest in technology stocks, particularly those involved in artificial intelligence and renewable energy. However, it's important to consider market volatility and global economic factors in your investment decisions.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What should I consider when investing in tech stocks?\"}\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a398fc4f-c439-4050-a2e9-da807717318b",
      "metadata": {
        "id": "a398fc4f-c439-4050-a2e9-da807717318b"
      },
      "source": [
        "Perfect! Now, our model is currently sitting on the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05466436-f050-45c8-9f7e-23e8b9acf74b",
      "metadata": {
        "id": "05466436-f050-45c8-9f7e-23e8b9acf74b"
      },
      "outputs": [],
      "source": [
        "model.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c6628ed-1d58-49bf-8f18-f5365ad34426",
      "metadata": {
        "id": "7c6628ed-1d58-49bf-8f18-f5365ad34426"
      },
      "source": [
        "Whereas our inputs are on the CPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb986f71-a374-4606-88bd-f8aaff754f45",
      "metadata": {
        "id": "cb986f71-a374-4606-88bd-f8aaff754f45"
      },
      "outputs": [],
      "source": [
        "encodeds.device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa895911-1fa0-4a44-9d6e-478eff4154ac",
      "metadata": {
        "id": "fa895911-1fa0-4a44-9d6e-478eff4154ac"
      },
      "source": [
        "Every PyTorch model expects the inputs to be on the same device as the model. If the model is a race car and we are the passengers, we need to be in the race car before we can get going! Let's move the inputs to the correct device before generating:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1eb655f-d621-4a58-b400-dd78b1acf762",
      "metadata": {
        "id": "c1eb655f-d621-4a58-b400-dd78b1acf762"
      },
      "outputs": [],
      "source": [
        "model_inputs = encodeds.to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e1e81e4-22d2-49ff-81a7-a1b3ecd4d47a",
      "metadata": {
        "id": "9e1e81e4-22d2-49ff-81a7-a1b3ecd4d47a"
      },
      "source": [
        "Now that we've pre-processed our inputs, we can generate our response using the model in much the same way as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc53409-a5e5-4e3d-8129-209be763f04d",
      "metadata": {
        "id": "edc53409-a5e5-4e3d-8129-209be763f04d"
      },
      "outputs": [],
      "source": [
        "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True)\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(generated_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af8fc47-4023-4120-86d7-4c31a12435bc",
      "metadata": {
        "id": "3af8fc47-4023-4120-86d7-4c31a12435bc"
      },
      "source": [
        "Looks like a very reasonable response! We can see the change between the user input and the assistant response marked by the `[INST]` and `[/INST]` tokens. This was handled automatically for us by the tokenizer. Note that we've generated with [*sampling*](https://huggingface.co/blog/how-to-generate#sampling), so there's an element of randomness in the generations, meaining each one will be different from the last. Feel free to re-run generation a few times to get a feel for the kind of variety that sampling brings.\n",
        "\n",
        "> **Note:** the responses generated by the model are purely for demonstration purposes. They should not be taken as financial recommendations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c85bc5e-faf6-4f35-b22f-212fdd3eda9e",
      "metadata": {
        "id": "0c85bc5e-faf6-4f35-b22f-212fdd3eda9e"
      },
      "source": [
        "## Streaming Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065190a3-4062-45f3-bf91-918e7411b8dc",
      "metadata": {
        "id": "065190a3-4062-45f3-bf91-918e7411b8dc"
      },
      "source": [
        "While the generated response looks good, we had to wait a significant amount of time for the model to finish generating. We know from our knowledge of Transformer models that they generate in an *auto-regressive* fashion, that is, one token at a time:\n",
        "\n",
        "<figure class=\"image table text-center m-0 w-full\">\n",
        "    <video\n",
        "        style=\"max-width: 90%; margin: auto;\"\n",
        "        autoplay loop muted playsinline\n",
        "        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov\"\n",
        "    ></video>\n",
        "</figure>\n",
        "\n",
        "Rather than waiting for the entire text sequence to finish generating, we can print each token as it is generated. While this doesn't change the overall latency of the model (the total generation time is the same as before), the *percieved* latency is lower, since the first generated token is returned to the user as soon as it's ready.\n",
        "\n",
        "Transformers provides a [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer) class to easily print the predictions on-the-fly. We simply have to pass the streamer to our generate function, and the rest is taken care for us. Let's run an example using the text streamer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be7896d-160a-40f5-a5f0-72b382bea2d9",
      "metadata": {
        "id": "2be7896d-160a-40f5-a5f0-72b382bea2d9"
      },
      "outputs": [],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True, streamer=streamer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c0aac2a-5458-4f33-a54c-f39378dd184c",
      "metadata": {
        "id": "9c0aac2a-5458-4f33-a54c-f39378dd184c"
      },
      "source": [
        "Interacting with the LLM this way already feels more personalised, as we watch it generate each token at a time. It also feels faster to the user, as we get the first output as soon as it is ready."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Prompt Engineering\n",
        "\n",
        "We can ask the model to perform a more advanced task by modifying the prompt. In the following example, we'll provide the model with a summary of [Apple's Q3 Earnings Report](https://www.apple.com/uk/newsroom/2023/08/apple-reports-third-quarter-results/), and ask it to provide a summary of the potential implications on the US economy. This is an example of *prompt engineering*, where we control the functionality of the model by changing the form of the input prompt:"
      ],
      "metadata": {
        "id": "C9ZUB02idhDs"
      },
      "id": "C9ZUB02idhDs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0bf899-624c-4cd3-b968-d5b12d5e92ac",
      "metadata": {
        "id": "7b0bf899-624c-4cd3-b968-d5b12d5e92ac"
      },
      "outputs": [],
      "source": [
        "APPLE_EARNINGS = (\n",
        "    \"Apple today announced financial results for its fiscal 2023 third quarter ended July 1, 2023. The Company posted quarterly revenue of $81.8 billion, down 1 percent year over year, and quarterly earnings per diluted share of $1.26, up 5 percent year over year. \"\n",
        "    \"'We are happy to report that we had an all-time revenue record in Services during the June quarter, driven by over 1 billion paid subscriptions, and we saw continued strength in emerging markets thanks to robust sales of iPhone,' said Tim Cook, Appleâ€™s CEO. 'From education to the environment, we are continuing to advance our values, while championing innovation that enriches the lives of our customers and leaves the world better than we found it.'\"\n",
        "    \"'Our June quarter year-over-year business performance improved from the March quarter, and our installed base of active devices reached an all-time high in every geographic segment,' said Luca Maestri, Appleâ€™s CFO. 'During the quarter, we generated very strong operating cash flow of $26 billion, returned over $24 billion to our shareholders, and continued to invest in our long-term growth plans.\"\n",
        "    \"Appleâ€™s board of directors has declared a cash dividend of $0.24 per share of the Companyâ€™s common stock. The dividend is payable on August 17, 2023 to shareholders of record as of the close of business on August 14, 2023.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": (\n",
        "        \"I have the latest earnings report of Apple. Based on this report's financial performance indicators, including revenue, profits, and growth projections, \"\n",
        "        \"can you analyze and advise on the potential impact this company's performance might have on the broader US economy? \"\n",
        "        \"Please consider factors such as the company's market influence, sector trends, and any ripple effects that might be observed in related industries \"\n",
        "        f\"or the economy as a whole. Here is the report: {APPLE_EARNINGS}\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
        "model_inputs = encodeds.to(model.device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_ids = model.generate(model_inputs, max_new_tokens=1024, do_sample=True, streamer=streamer)"
      ],
      "metadata": {
        "id": "NKXad60wfHgl"
      },
      "id": "NKXad60wfHgl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9c2ada74-36c1-49d1-9d64-7452af25488f",
      "metadata": {
        "id": "9c2ada74-36c1-49d1-9d64-7452af25488f"
      },
      "source": [
        "## Gradio Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd86cf1d-49d6-4e92-b63b-0e492cc8fe4d",
      "metadata": {
        "id": "dd86cf1d-49d6-4e92-b63b-0e492cc8fe4d"
      },
      "source": [
        "While it's interesting for us learners to interact with the model through Python code, it's by no means the easiest way to share our model with others. To finish this section, we'll demonstrate how the Mistral model can be wrapped into a simple chat interface and shared with anyone in the community.\n",
        "\n",
        "To achieve this, we'll use the [Gradio library](https://www.gradio.app). Gradio is an open-source Python library that simplifies the creation of web interfaces for machine learning models. It allows developers and researchers to quickly build and share customisable UI components for model testing and feedback, enhancing the accessibility and usability of AI models.\n",
        "\n",
        "First, we'll define an end-to-end function that takes the user's message and the chat history, and generates a response. Largely speaking, this simply requires concatenating the three stages of prediction:\n",
        "1. Tokenizer (pre-process) the message and chat history\n",
        "2. Define the generation arguments\n",
        "3. Generate the predicted ids and stream the output\n",
        "\n",
        "We'll make a small modification to our streamer: instead of using the base [`TextStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextStreamer), we'll use the [`TextIteratorStreamer`](https://huggingface.co/docs/transformers/internal/generation_utils#transformers.TextIteratorStreamer), which returns the generated ids in a non-blocking way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b62bebe7-27da-4255-8429-bdcd0033b7b8",
      "metadata": {
        "id": "b62bebe7-27da-4255-8429-bdcd0033b7b8"
      },
      "outputs": [],
      "source": [
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "MAX_INPUT_TOKEN_LENGTH = 4096\n",
        "\n",
        "def generate(message, chat_history):\n",
        "    # Step 1: pre-process the inputs\n",
        "    conversation = []\n",
        "    for user, assistant in chat_history:\n",
        "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
        "\n",
        "    # in-case our inputs exceed the maximum length, we might need to cut them\n",
        "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
        "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
        "        gr.Warning(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
        "\n",
        "    input_ids = input_ids.to(model.device)\n",
        "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "    # Step 2: define generation arguments\n",
        "    generate_kwargs = dict(\n",
        "        {\"input_ids\": input_ids},\n",
        "        streamer=streamer,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "    )\n",
        "\n",
        "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
        "    t.start()\n",
        "\n",
        "    # Step 3: generate and stream outputs\n",
        "    outputs = \"\"\n",
        "    for text in streamer:\n",
        "        outputs += text\n",
        "        yield outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc5112d2-a068-4acc-838d-b07c1052b8c8",
      "metadata": {
        "id": "bc5112d2-a068-4acc-838d-b07c1052b8c8"
      },
      "source": [
        "With the generation function defined, we can now create a Gradio demo in just 3 lines of additional code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d407a6d9-9eef-4a27-97c4-89dec1d10bad",
      "metadata": {
        "id": "d407a6d9-9eef-4a27-97c4-89dec1d10bad"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "chat_interface = gr.ChatInterface(generate)\n",
        "chat_interface.queue().launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94af8af6-f499-400d-a784-9277be0e6c5a",
      "metadata": {
        "id": "94af8af6-f499-400d-a784-9277be0e6c5a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}